{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources** :\n",
    "\n",
    "*Matrix Factorization techniques for Recommender Systems*, Koren (2009)    \n",
    "https://www.inf.unibz.it/~ricci/ISR/papers/ieeecomputer.pdf\n",
    "\n",
    "Hands on Machine Learning with scikit-learn and tensorflow:             \n",
    "https://www.lpsm.paris/pageperso/has/source/Hand-on-ML.pdf\n",
    "\n",
    "The movieLens dataset:                                                \n",
    "https://grouplens.org/datasets/movielens/ \n",
    "\n",
    "Keras Sequential API doc :                                            \n",
    "https://keras.io/models/model/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems : collaborative filtering via matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you wonder how Netflix is able to recommend you movies despite it doesn't know anything about you but the ratings you gave to the movies you watched ? This is what we are going to explore during this 3 days machine learning module.\n",
    "\n",
    "First off, let's learn about what are recommender system, collaborative filtering and matrix factorization techniques, which are all very well introduced in Koren's 2009 famous article : *Matrix Factorization techniques for Recommender Systems* : https://www.inf.unibz.it/~ricci/ISR/papers/ieeecomputer.pdf . Read the 4 first pages (up to section *adding biases* included). \n",
    "\n",
    "Through this notebook we are going to re-implement the model described in the pages you read, and apply it to a classic movie ratings dataset coming from the website *movieLens*. To do so, we will use a powerful deep learning python library called *Keras*, that makes it easy to train complex models based on linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we are going to use the movieLens dataset, that contains data from the movie recommending website movielens. The data is a subset of ratings from 0 to 5 given by some users of the website to a subset of movies. You can read more about it here (we are using the latest small dataset) : https://grouplens.org/datasets/movielens/ , and in the *README* file that is in the *data/ml-latest-small/* folder.\n",
    "\n",
    "Load the ratings data from the `ratings.csv` file into a dataframe. The userId and movieId provided in the file don't start from 0, and are not contiguous (i.e. there are missing indexes).\n",
    "\n",
    "Re-index the user and movie ids to indexes going from 0 to `nb_users` and 0 to `nb_movies` respectively, by building two dictionnaries `user_ids_map` and `movie_ids_map` that maps the file ids to your new ids. \n",
    "And finally, split the rows of this dataframe in a random 90%/10% train/test sets.\n",
    "\n",
    "To do so, fill the `get_train_test_sets` function below, and respect the returned objects structures that are described in the docstring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - toutes les cellules avant la cellule avec la grosse fonction sont des tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on charge notre dataset - à implémenter dans la fonction\n",
    "ratings_csv=pd.read_csv('../data/ml-latest-small/ratings.csv')\n",
    "ratings_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv.userId.nunique() # affiche le nombre de valeurs uniques. Ici on ira de 0 à 610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "       365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "       391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
       "       404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
       "       417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
       "       430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
       "       443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "       456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "       469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
       "       482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494,\n",
       "       495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507,\n",
       "       508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520,\n",
       "       521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n",
       "       534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546,\n",
       "       547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
       "       560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572,\n",
       "       573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585,\n",
       "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598,\n",
       "       599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv.userId.unique() # affiche les valeurs uniques de userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv.movieId.nunique() # pour les movieId on devra aller de 0 à 9723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     1,      3,      6, ..., 160836, 163937, 163981])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv.movieId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 3: 1,\n",
       " 6: 2,\n",
       " 47: 3,\n",
       " 50: 4,\n",
       " 70: 5,\n",
       " 101: 6,\n",
       " 110: 7,\n",
       " 151: 8,\n",
       " 157: 9,\n",
       " 163: 10,\n",
       " 216: 11,\n",
       " 223: 12,\n",
       " 231: 13,\n",
       " 235: 14,\n",
       " 260: 15,\n",
       " 296: 16,\n",
       " 316: 17,\n",
       " 333: 18,\n",
       " 349: 19,\n",
       " 356: 20,\n",
       " 362: 21,\n",
       " 367: 22,\n",
       " 423: 23,\n",
       " 441: 24,\n",
       " 457: 25,\n",
       " 480: 26,\n",
       " 500: 27,\n",
       " 527: 28,\n",
       " 543: 29,\n",
       " 552: 30,\n",
       " 553: 31,\n",
       " 590: 32,\n",
       " 592: 33,\n",
       " 593: 34,\n",
       " 596: 35,\n",
       " 608: 36,\n",
       " 648: 37,\n",
       " 661: 38,\n",
       " 673: 39,\n",
       " 733: 40,\n",
       " 736: 41,\n",
       " 780: 42,\n",
       " 804: 43,\n",
       " 919: 44,\n",
       " 923: 45,\n",
       " 940: 46,\n",
       " 943: 47,\n",
       " 954: 48,\n",
       " 1009: 49,\n",
       " 1023: 50,\n",
       " 1024: 51,\n",
       " 1025: 52,\n",
       " 1029: 53,\n",
       " 1030: 54,\n",
       " 1031: 55,\n",
       " 1032: 56,\n",
       " 1042: 57,\n",
       " 1049: 58,\n",
       " 1060: 59,\n",
       " 1073: 60,\n",
       " 1080: 61,\n",
       " 1089: 62,\n",
       " 1090: 63,\n",
       " 1092: 64,\n",
       " 1097: 65,\n",
       " 1127: 66,\n",
       " 1136: 67,\n",
       " 1196: 68,\n",
       " 1197: 69,\n",
       " 1198: 70,\n",
       " 1206: 71,\n",
       " 1208: 72,\n",
       " 1210: 73,\n",
       " 1213: 74,\n",
       " 1214: 75,\n",
       " 1219: 76,\n",
       " 1220: 77,\n",
       " 1222: 78,\n",
       " 1224: 79,\n",
       " 1226: 80,\n",
       " 1240: 81,\n",
       " 1256: 82,\n",
       " 1258: 83,\n",
       " 1265: 84,\n",
       " 1270: 85,\n",
       " 1275: 86,\n",
       " 1278: 87,\n",
       " 1282: 88,\n",
       " 1291: 89,\n",
       " 1298: 90,\n",
       " 1348: 91,\n",
       " 1377: 92,\n",
       " 1396: 93,\n",
       " 1408: 94,\n",
       " 1445: 95,\n",
       " 1473: 96,\n",
       " 1500: 97,\n",
       " 1517: 98,\n",
       " 1552: 99,\n",
       " 1573: 100,\n",
       " 1580: 101,\n",
       " 1587: 102,\n",
       " 1617: 103,\n",
       " 1620: 104,\n",
       " 1625: 105,\n",
       " 1644: 106,\n",
       " 1676: 107,\n",
       " 1732: 108,\n",
       " 1777: 109,\n",
       " 1793: 110,\n",
       " 1804: 111,\n",
       " 1805: 112,\n",
       " 1920: 113,\n",
       " 1927: 114,\n",
       " 1954: 115,\n",
       " 1967: 116,\n",
       " 2000: 117,\n",
       " 2005: 118,\n",
       " 2012: 119,\n",
       " 2018: 120,\n",
       " 2028: 121,\n",
       " 2033: 122,\n",
       " 2046: 123,\n",
       " 2048: 124,\n",
       " 2054: 125,\n",
       " 2058: 126,\n",
       " 2078: 127,\n",
       " 2090: 128,\n",
       " 2093: 129,\n",
       " 2094: 130,\n",
       " 2096: 131,\n",
       " 2099: 132,\n",
       " 2105: 133,\n",
       " 2115: 134,\n",
       " 2116: 135,\n",
       " 2137: 136,\n",
       " 2139: 137,\n",
       " 2141: 138,\n",
       " 2143: 139,\n",
       " 2161: 140,\n",
       " 2174: 141,\n",
       " 2193: 142,\n",
       " 2253: 143,\n",
       " 2268: 144,\n",
       " 2273: 145,\n",
       " 2291: 146,\n",
       " 2329: 147,\n",
       " 2338: 148,\n",
       " 2353: 149,\n",
       " 2366: 150,\n",
       " 2387: 151,\n",
       " 2389: 152,\n",
       " 2395: 153,\n",
       " 2406: 154,\n",
       " 2414: 155,\n",
       " 2427: 156,\n",
       " 2450: 157,\n",
       " 2459: 158,\n",
       " 2470: 159,\n",
       " 2478: 160,\n",
       " 2492: 161,\n",
       " 2502: 162,\n",
       " 2528: 163,\n",
       " 2529: 164,\n",
       " 2542: 165,\n",
       " 2571: 166,\n",
       " 2580: 167,\n",
       " 2596: 168,\n",
       " 2616: 169,\n",
       " 2617: 170,\n",
       " 2628: 171,\n",
       " 2640: 172,\n",
       " 2641: 173,\n",
       " 2644: 174,\n",
       " 2648: 175,\n",
       " 2654: 176,\n",
       " 2657: 177,\n",
       " 2692: 178,\n",
       " 2700: 179,\n",
       " 2716: 180,\n",
       " 2761: 181,\n",
       " 2797: 182,\n",
       " 2826: 183,\n",
       " 2858: 184,\n",
       " 2872: 185,\n",
       " 2899: 186,\n",
       " 2916: 187,\n",
       " 2944: 188,\n",
       " 2947: 189,\n",
       " 2948: 190,\n",
       " 2949: 191,\n",
       " 2959: 192,\n",
       " 2985: 193,\n",
       " 2987: 194,\n",
       " 2991: 195,\n",
       " 2993: 196,\n",
       " 2997: 197,\n",
       " 3033: 198,\n",
       " 3034: 199,\n",
       " 3052: 200,\n",
       " 3053: 201,\n",
       " 3062: 202,\n",
       " 3147: 203,\n",
       " 3168: 204,\n",
       " 3176: 205,\n",
       " 3243: 206,\n",
       " 3247: 207,\n",
       " 3253: 208,\n",
       " 3273: 209,\n",
       " 3386: 210,\n",
       " 3439: 211,\n",
       " 3440: 212,\n",
       " 3441: 213,\n",
       " 3448: 214,\n",
       " 3450: 215,\n",
       " 3479: 216,\n",
       " 3489: 217,\n",
       " 3527: 218,\n",
       " 3578: 219,\n",
       " 3617: 220,\n",
       " 3639: 221,\n",
       " 3671: 222,\n",
       " 3702: 223,\n",
       " 3703: 224,\n",
       " 3729: 225,\n",
       " 3740: 226,\n",
       " 3744: 227,\n",
       " 3793: 228,\n",
       " 3809: 229,\n",
       " 4006: 230,\n",
       " 5060: 231,\n",
       " 318: 232,\n",
       " 1704: 233,\n",
       " 6874: 234,\n",
       " 8798: 235,\n",
       " 46970: 236,\n",
       " 48516: 237,\n",
       " 58559: 238,\n",
       " 60756: 239,\n",
       " 68157: 240,\n",
       " 71535: 241,\n",
       " 74458: 242,\n",
       " 77455: 243,\n",
       " 79132: 244,\n",
       " 80489: 245,\n",
       " 80906: 246,\n",
       " 86345: 247,\n",
       " 89774: 248,\n",
       " 91529: 249,\n",
       " 91658: 250,\n",
       " 99114: 251,\n",
       " 106782: 252,\n",
       " 109487: 253,\n",
       " 112552: 254,\n",
       " 114060: 255,\n",
       " 115713: 256,\n",
       " 122882: 257,\n",
       " 131724: 258,\n",
       " 31: 259,\n",
       " 647: 260,\n",
       " 688: 261,\n",
       " 720: 262,\n",
       " 849: 263,\n",
       " 914: 264,\n",
       " 1093: 265,\n",
       " 1124: 266,\n",
       " 1263: 267,\n",
       " 1272: 268,\n",
       " 1302: 269,\n",
       " 1371: 270,\n",
       " 2080: 271,\n",
       " 2288: 272,\n",
       " 2424: 273,\n",
       " 2851: 274,\n",
       " 3024: 275,\n",
       " 3210: 276,\n",
       " 3949: 277,\n",
       " 4518: 278,\n",
       " 5048: 279,\n",
       " 5181: 280,\n",
       " 5746: 281,\n",
       " 5764: 282,\n",
       " 5919: 283,\n",
       " 6238: 284,\n",
       " 6835: 285,\n",
       " 7899: 286,\n",
       " 7991: 287,\n",
       " 26409: 288,\n",
       " 70946: 289,\n",
       " 72378: 290,\n",
       " 21: 291,\n",
       " 32: 292,\n",
       " 45: 293,\n",
       " 52: 294,\n",
       " 58: 295,\n",
       " 106: 296,\n",
       " 125: 297,\n",
       " 126: 298,\n",
       " 162: 299,\n",
       " 171: 300,\n",
       " 176: 301,\n",
       " 190: 302,\n",
       " 215: 303,\n",
       " 222: 304,\n",
       " 232: 305,\n",
       " 247: 306,\n",
       " 265: 307,\n",
       " 319: 308,\n",
       " 342: 309,\n",
       " 345: 310,\n",
       " 348: 311,\n",
       " 351: 312,\n",
       " 357: 313,\n",
       " 368: 314,\n",
       " 417: 315,\n",
       " 450: 316,\n",
       " 475: 317,\n",
       " 492: 318,\n",
       " 509: 319,\n",
       " 538: 320,\n",
       " 539: 321,\n",
       " 588: 322,\n",
       " 595: 323,\n",
       " 599: 324,\n",
       " 708: 325,\n",
       " 759: 326,\n",
       " 800: 327,\n",
       " 892: 328,\n",
       " 898: 329,\n",
       " 899: 330,\n",
       " 902: 331,\n",
       " 904: 332,\n",
       " 908: 333,\n",
       " 910: 334,\n",
       " 912: 335,\n",
       " 920: 336,\n",
       " 930: 337,\n",
       " 937: 338,\n",
       " 1046: 339,\n",
       " 1057: 340,\n",
       " 1077: 341,\n",
       " 1079: 342,\n",
       " 1084: 343,\n",
       " 1086: 344,\n",
       " 1094: 345,\n",
       " 1103: 346,\n",
       " 1179: 347,\n",
       " 1183: 348,\n",
       " 1188: 349,\n",
       " 1199: 350,\n",
       " 1203: 351,\n",
       " 1211: 352,\n",
       " 1225: 353,\n",
       " 1250: 354,\n",
       " 1259: 355,\n",
       " 1266: 356,\n",
       " 1279: 357,\n",
       " 1283: 358,\n",
       " 1288: 359,\n",
       " 1304: 360,\n",
       " 1391: 361,\n",
       " 1449: 362,\n",
       " 1466: 363,\n",
       " 1597: 364,\n",
       " 1641: 365,\n",
       " 1719: 366,\n",
       " 1733: 367,\n",
       " 1734: 368,\n",
       " 1834: 369,\n",
       " 1860: 370,\n",
       " 1883: 371,\n",
       " 1885: 372,\n",
       " 1892: 373,\n",
       " 1895: 374,\n",
       " 1907: 375,\n",
       " 1914: 376,\n",
       " 1916: 377,\n",
       " 1923: 378,\n",
       " 1947: 379,\n",
       " 1966: 380,\n",
       " 1968: 381,\n",
       " 2019: 382,\n",
       " 2076: 383,\n",
       " 2109: 384,\n",
       " 2145: 385,\n",
       " 2150: 386,\n",
       " 2186: 387,\n",
       " 2203: 388,\n",
       " 2204: 389,\n",
       " 2282: 390,\n",
       " 2324: 391,\n",
       " 2336: 392,\n",
       " 2351: 393,\n",
       " 2359: 394,\n",
       " 2390: 395,\n",
       " 2467: 396,\n",
       " 2583: 397,\n",
       " 2599: 398,\n",
       " 2683: 399,\n",
       " 2712: 400,\n",
       " 2762: 401,\n",
       " 2763: 402,\n",
       " 2770: 403,\n",
       " 2791: 404,\n",
       " 2843: 405,\n",
       " 2874: 406,\n",
       " 2921: 407,\n",
       " 2926: 408,\n",
       " 2973: 409,\n",
       " 3044: 410,\n",
       " 3060: 411,\n",
       " 3079: 412,\n",
       " 3083: 413,\n",
       " 3160: 414,\n",
       " 3175: 415,\n",
       " 3204: 416,\n",
       " 3255: 417,\n",
       " 3317: 418,\n",
       " 3358: 419,\n",
       " 3365: 420,\n",
       " 3408: 421,\n",
       " 3481: 422,\n",
       " 3508: 423,\n",
       " 3538: 424,\n",
       " 3591: 425,\n",
       " 3788: 426,\n",
       " 3851: 427,\n",
       " 3897: 428,\n",
       " 3911: 429,\n",
       " 3967: 430,\n",
       " 3996: 431,\n",
       " 4002: 432,\n",
       " 4014: 433,\n",
       " 4020: 434,\n",
       " 4021: 435,\n",
       " 4027: 436,\n",
       " 4029: 437,\n",
       " 4033: 438,\n",
       " 4034: 439,\n",
       " 4074: 440,\n",
       " 4121: 441,\n",
       " 4144: 442,\n",
       " 4166: 443,\n",
       " 4226: 444,\n",
       " 4239: 445,\n",
       " 4246: 446,\n",
       " 4252: 447,\n",
       " 4260: 448,\n",
       " 4273: 449,\n",
       " 4308: 450,\n",
       " 4347: 451,\n",
       " 4381: 452,\n",
       " 4641: 453,\n",
       " 4741: 454,\n",
       " 4765: 455,\n",
       " 4881: 456,\n",
       " 4896: 457,\n",
       " 4902: 458,\n",
       " 4967: 459,\n",
       " 34: 460,\n",
       " 36: 461,\n",
       " 39: 462,\n",
       " 150: 463,\n",
       " 153: 464,\n",
       " 253: 465,\n",
       " 261: 466,\n",
       " 266: 467,\n",
       " 290: 468,\n",
       " 300: 469,\n",
       " 344: 470,\n",
       " 364: 471,\n",
       " 380: 472,\n",
       " 410: 473,\n",
       " 474: 474,\n",
       " 515: 475,\n",
       " 531: 476,\n",
       " 534: 477,\n",
       " 589: 478,\n",
       " 594: 479,\n",
       " 597: 480,\n",
       " 2: 481,\n",
       " 4: 482,\n",
       " 5: 483,\n",
       " 7: 484,\n",
       " 8: 485,\n",
       " 10: 486,\n",
       " 11: 487,\n",
       " 13: 488,\n",
       " 15: 489,\n",
       " 16: 490,\n",
       " 17: 491,\n",
       " 19: 492,\n",
       " 22: 493,\n",
       " 24: 494,\n",
       " 25: 495,\n",
       " 26: 496,\n",
       " 27: 497,\n",
       " 41: 498,\n",
       " 43: 499,\n",
       " 46: 500,\n",
       " 54: 501,\n",
       " 60: 502,\n",
       " 61: 503,\n",
       " 62: 504,\n",
       " 65: 505,\n",
       " 66: 506,\n",
       " 76: 507,\n",
       " 79: 508,\n",
       " 86: 509,\n",
       " 87: 510,\n",
       " 88: 511,\n",
       " 89: 512,\n",
       " 92: 513,\n",
       " 93: 514,\n",
       " 95: 515,\n",
       " 100: 516,\n",
       " 102: 517,\n",
       " 104: 518,\n",
       " 105: 519,\n",
       " 112: 520,\n",
       " 113: 521,\n",
       " 135: 522,\n",
       " 140: 523,\n",
       " 141: 524,\n",
       " 145: 525,\n",
       " 146: 526,\n",
       " 158: 527,\n",
       " 159: 528,\n",
       " 160: 529,\n",
       " 161: 530,\n",
       " 165: 531,\n",
       " 168: 532,\n",
       " 170: 533,\n",
       " 174: 534,\n",
       " 177: 535,\n",
       " 179: 536,\n",
       " 180: 537,\n",
       " 181: 538,\n",
       " 185: 539,\n",
       " 186: 540,\n",
       " 189: 541,\n",
       " 191: 542,\n",
       " 195: 543,\n",
       " 196: 544,\n",
       " 201: 545,\n",
       " 204: 546,\n",
       " 205: 547,\n",
       " 207: 548,\n",
       " 208: 549,\n",
       " 209: 550,\n",
       " 210: 551,\n",
       " 212: 552,\n",
       " 217: 553,\n",
       " 218: 554,\n",
       " 219: 555,\n",
       " 224: 556,\n",
       " 225: 557,\n",
       " 230: 558,\n",
       " 234: 559,\n",
       " 236: 560,\n",
       " 237: 561,\n",
       " 239: 562,\n",
       " 240: 563,\n",
       " 243: 564,\n",
       " 248: 565,\n",
       " 250: 566,\n",
       " 251: 567,\n",
       " 252: 568,\n",
       " 254: 569,\n",
       " 256: 570,\n",
       " 257: 571,\n",
       " 258: 572,\n",
       " 262: 573,\n",
       " 267: 574,\n",
       " 270: 575,\n",
       " 271: 576,\n",
       " 273: 577,\n",
       " 274: 578,\n",
       " 276: 579,\n",
       " 277: 580,\n",
       " 279: 581,\n",
       " 281: 582,\n",
       " 282: 583,\n",
       " 288: 584,\n",
       " 289: 585,\n",
       " 291: 586,\n",
       " 292: 587,\n",
       " 293: 588,\n",
       " 302: 589,\n",
       " 303: 590,\n",
       " 304: 591,\n",
       " 310: 592,\n",
       " 312: 593,\n",
       " 313: 594,\n",
       " 314: 595,\n",
       " 315: 596,\n",
       " 317: 597,\n",
       " 327: 598,\n",
       " 329: 599,\n",
       " 330: 600,\n",
       " 332: 601,\n",
       " 336: 602,\n",
       " 337: 603,\n",
       " 339: 604,\n",
       " 340: 605,\n",
       " 343: 606,\n",
       " 347: 607,\n",
       " 350: 608,\n",
       " 352: 609,\n",
       " 353: 610,\n",
       " 354: 611,\n",
       " 355: 612,\n",
       " 358: 613,\n",
       " 359: 614,\n",
       " 360: 615,\n",
       " 361: 616,\n",
       " 366: 617,\n",
       " 370: 618,\n",
       " 371: 619,\n",
       " 374: 620,\n",
       " 377: 621,\n",
       " 378: 622,\n",
       " 381: 623,\n",
       " 382: 624,\n",
       " 383: 625,\n",
       " 405: 626,\n",
       " 412: 627,\n",
       " 415: 628,\n",
       " 416: 629,\n",
       " 419: 630,\n",
       " 426: 631,\n",
       " 432: 632,\n",
       " 434: 633,\n",
       " 435: 634,\n",
       " 437: 635,\n",
       " 440: 636,\n",
       " 445: 637,\n",
       " 454: 638,\n",
       " 455: 639,\n",
       " 458: 640,\n",
       " 460: 641,\n",
       " 466: 642,\n",
       " 468: 643,\n",
       " 469: 644,\n",
       " 472: 645,\n",
       " 477: 646,\n",
       " 485: 647,\n",
       " 489: 648,\n",
       " 490: 649,\n",
       " 491: 650,\n",
       " 493: 651,\n",
       " 494: 652,\n",
       " 497: 653,\n",
       " 502: 654,\n",
       " 505: 655,\n",
       " 508: 656,\n",
       " 510: 657,\n",
       " 516: 658,\n",
       " 520: 659,\n",
       " 524: 660,\n",
       " 532: 661,\n",
       " 536: 662,\n",
       " 537: 663,\n",
       " 540: 664,\n",
       " 542: 665,\n",
       " 546: 666,\n",
       " 548: 667,\n",
       " 569: 668,\n",
       " 575: 669,\n",
       " 587: 670,\n",
       " 606: 671,\n",
       " 609: 672,\n",
       " 616: 673,\n",
       " 628: 674,\n",
       " 631: 675,\n",
       " 637: 676,\n",
       " 640: 677,\n",
       " 662: 678,\n",
       " 667: 679,\n",
       " 694: 680,\n",
       " 697: 681,\n",
       " 700: 682,\n",
       " 704: 683,\n",
       " 709: 684,\n",
       " 710: 685,\n",
       " 711: 686,\n",
       " 719: 687,\n",
       " 747: 688,\n",
       " 762: 689,\n",
       " 765: 690,\n",
       " 775: 691,\n",
       " 783: 692,\n",
       " 795: 693,\n",
       " 799: 694,\n",
       " 801: 695,\n",
       " 802: 696,\n",
       " 818: 697,\n",
       " 830: 698,\n",
       " 835: 699,\n",
       " 837: 700,\n",
       " 838: 701,\n",
       " 839: 702,\n",
       " 842: 703,\n",
       " 848: 704,\n",
       " 852: 705,\n",
       " 867: 706,\n",
       " 880: 707,\n",
       " 881: 708,\n",
       " 888: 709,\n",
       " 891: 710,\n",
       " 979: 711,\n",
       " 981: 712,\n",
       " 986: 713,\n",
       " 991: 714,\n",
       " 996: 715,\n",
       " 999: 716,\n",
       " 1004: 717,\n",
       " 1006: 718,\n",
       " 1061: 719,\n",
       " 1064: 720,\n",
       " 1082: 721,\n",
       " 750: 722,\n",
       " 924: 723,\n",
       " 1101: 724,\n",
       " 1246: 725,\n",
       " 1584: 726,\n",
       " 1610: 727,\n",
       " 1682: 728,\n",
       " 1784: 729,\n",
       " 1917: 730,\n",
       " 2671: 731,\n",
       " 2688: 732,\n",
       " 2701: 733,\n",
       " 2717: 734,\n",
       " 3114: 735,\n",
       " 3354: 736,\n",
       " 3623: 737,\n",
       " 3869: 738,\n",
       " 3916: 739,\n",
       " 3977: 740,\n",
       " 3994: 741,\n",
       " 4018: 742,\n",
       " 4223: 743,\n",
       " 4306: 744,\n",
       " 4310: 745,\n",
       " 4370: 746,\n",
       " 4643: 747,\n",
       " 4700: 748,\n",
       " 4844: 749,\n",
       " 4874: 750,\n",
       " 4886: 751,\n",
       " 4963: 752,\n",
       " 4993: 753,\n",
       " 4995: 754,\n",
       " 5218: 755,\n",
       " 5349: 756,\n",
       " 5378: 757,\n",
       " 5445: 758,\n",
       " 5459: 759,\n",
       " 5464: 760,\n",
       " 5502: 761,\n",
       " 5618: 762,\n",
       " 5816: 763,\n",
       " 5952: 764,\n",
       " 5989: 765,\n",
       " 5991: 766,\n",
       " 6333: 767,\n",
       " 6365: 768,\n",
       " 6534: 769,\n",
       " 6539: 770,\n",
       " 6863: 771,\n",
       " 6934: 772,\n",
       " 7143: 773,\n",
       " 7153: 774,\n",
       " 7155: 775,\n",
       " 7445: 776,\n",
       " 8207: 777,\n",
       " 8360: 778,\n",
       " 8368: 779,\n",
       " 8373: 780,\n",
       " 8528: 781,\n",
       " 8636: 782,\n",
       " 8665: 783,\n",
       " 8666: 784,\n",
       " 8783: 785,\n",
       " 8808: 786,\n",
       " 8865: 787,\n",
       " 8870: 788,\n",
       " 8907: 789,\n",
       " 8908: 790,\n",
       " 8949: 791,\n",
       " 8957: 792,\n",
       " 8958: 793,\n",
       " 8961: 794,\n",
       " 8965: 795,\n",
       " 8970: 796,\n",
       " 8972: 797,\n",
       " 8984: 798,\n",
       " 27741: 799,\n",
       " 30812: 800,\n",
       " 30816: 801,\n",
       " 31878: 802,\n",
       " 32029: 803,\n",
       " 32031: 804,\n",
       " 32296: 805,\n",
       " 32587: 806,\n",
       " 33162: 807,\n",
       " 33493: 808,\n",
       " 33794: 809,\n",
       " 33836: 810,\n",
       " 34048: 811,\n",
       " 34319: 812,\n",
       " 37741: 813,\n",
       " 38388: 814,\n",
       " 42002: 815,\n",
       " 45499: 816,\n",
       " 45517: 817,\n",
       " 45668: 818,\n",
       " 45730: 819,\n",
       " 46530: 820,\n",
       " 48783: 821,\n",
       " 48997: 822,\n",
       " 49272: 823,\n",
       " 49278: 824,\n",
       " 49286: 825,\n",
       " 49824: 826,\n",
       " 586: 827,\n",
       " 187: 828,\n",
       " 627: 829,\n",
       " 922: 830,\n",
       " 1037: 831,\n",
       " 1095: 832,\n",
       " 1674: 833,\n",
       " 1987: 834,\n",
       " 2011: 835,\n",
       " 2023: 836,\n",
       " 2300: 837,\n",
       " 2877: 838,\n",
       " 2901: 839,\n",
       " 3173: 840,\n",
       " 3328: 841,\n",
       " 3735: 842,\n",
       " 4131: 843,\n",
       " 4558: 844,\n",
       " 5447: 845,\n",
       " 5451: 846,\n",
       " 5481: 847,\n",
       " 5507: 848,\n",
       " 5841: 849,\n",
       " 5843: 850,\n",
       " 5872: 851,\n",
       " 5890: 852,\n",
       " 5891: 853,\n",
       " 5893: 854,\n",
       " 5902: 855,\n",
       " 5956: 856,\n",
       " 5962: 857,\n",
       " 5965: 858,\n",
       " 5988: 859,\n",
       " 6001: 860,\n",
       " 6044: 861,\n",
       " 1028: 862,\n",
       " 1088: 863,\n",
       " 1247: 864,\n",
       " 1307: 865,\n",
       " 3882: 866,\n",
       " 4447: 867,\n",
       " 5066: 868,\n",
       " 5377: 869,\n",
       " 5620: 870,\n",
       " 5943: 871,\n",
       " 5957: 872,\n",
       " 6155: 873,\n",
       " 6266: 874,\n",
       " 6377: 875,\n",
       " 6535: 876,\n",
       " 6942: 877,\n",
       " 7149: 878,\n",
       " 7151: 879,\n",
       " 7154: 880,\n",
       " 7169: 881,\n",
       " 7293: 882,\n",
       " 7375: 883,\n",
       " 7451: 884,\n",
       " 7458: 885,\n",
       " 8529: 886,\n",
       " 8533: 887,\n",
       " 8869: 888,\n",
       " 8969: 889,\n",
       " 30749: 890,\n",
       " 31433: 891,\n",
       " 31685: 892,\n",
       " 33145: 893,\n",
       " 33679: 894,\n",
       " 40629: 895,\n",
       " 40819: 896,\n",
       " 41285: 897,\n",
       " 47099: 898,\n",
       " 51662: 899,\n",
       " 51705: 900,\n",
       " 51834: 901,\n",
       " 54286: 902,\n",
       " 56367: 903,\n",
       " 56949: 904,\n",
       " 58047: 905,\n",
       " 59333: 906,\n",
       " 59421: 907,\n",
       " 60397: 908,\n",
       " 60950: 909,\n",
       " 61250: 910,\n",
       " 63113: 911,\n",
       " 63992: 912,\n",
       " 64969: 913,\n",
       " 66203: 914,\n",
       " 68954: 915,\n",
       " 69406: 916,\n",
       " 69844: 917,\n",
       " 70183: 918,\n",
       " 70293: 919,\n",
       " 71579: 920,\n",
       " 72011: 921,\n",
       " 72330: 922,\n",
       " 72407: 923,\n",
       " 72720: 924,\n",
       " 72737: 925,\n",
       " 72998: 926,\n",
       " 73017: 927,\n",
       " 74450: 928,\n",
       " 77841: 929,\n",
       " 78772: 930,\n",
       " 79091: 931,\n",
       " 80549: 932,\n",
       " 81784: 933,\n",
       " 81845: 934,\n",
       " 81847: 935,\n",
       " 82167: 936,\n",
       " 82499: 937,\n",
       " 84374: 938,\n",
       " 86548: 939,\n",
       " 87222: 940,\n",
       " 88163: 941,\n",
       " 88810: 942,\n",
       " 91104: 943,\n",
       " 92259: 944,\n",
       " 94070: 945,\n",
       " 95167: 946,\n",
       " 95449: 947,\n",
       " 95510: 948,\n",
       " 95543: 949,\n",
       " 96079: 950,\n",
       " 97024: 951,\n",
       " 97938: 952,\n",
       " 98203: 953,\n",
       " 103335: 954,\n",
       " 103339: 955,\n",
       " 104374: 956,\n",
       " 105211: 957,\n",
       " 106489: 958,\n",
       " 106696: 959,\n",
       " 107141: 960,\n",
       " 109374: 961,\n",
       " 109853: 962,\n",
       " 112006: 963,\n",
       " 113275: 964,\n",
       " 113394: 965,\n",
       " 119145: 966,\n",
       " 129428: 967,\n",
       " 136020: 968,\n",
       " 137595: 969,\n",
       " 140110: 970,\n",
       " 44: 971,\n",
       " 376: 972,\n",
       " 511: 973,\n",
       " 529: 974,\n",
       " 1100: 975,\n",
       " 1358: 976,\n",
       " 1370: 977,\n",
       " 1385: 978,\n",
       " 1438: 979,\n",
       " 1518: 980,\n",
       " 1586: 981,\n",
       " 1604: 982,\n",
       " 1608: 983,\n",
       " 1616: 984,\n",
       " 1687: 985,\n",
       " 1693: 986,\n",
       " 1721: 987,\n",
       " 1840: 988,\n",
       " 1882: 989,\n",
       " 1918: 990,\n",
       " 2002: 991,\n",
       " 2027: 992,\n",
       " 1357: 993,\n",
       " 1405: 994,\n",
       " 1876: 995,\n",
       " 2072: 996,\n",
       " 2100: 997,\n",
       " 2421: 998,\n",
       " 2485: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création d'un dico qui attribue de nouvelles valeurs aux movieId - à implémenter dans la fonction\n",
    "movie_ids_map={}\n",
    "index=0\n",
    "for movie in ratings_csv.movieId.unique():\n",
    "    movie_ids_map[movie]=index\n",
    "    index+=1\n",
    "movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A implémenter dans la fonction\n",
    "\n",
    "user_ids_map={}\n",
    "index=0\n",
    "for user in ratings_csv.userId.unique():\n",
    "    user_ids_map[user]=index\n",
    "    index+=1\n",
    "user_ids_map\n",
    "\n",
    "ratings_csv.userId.replace(to_replace=user_ids_map, value=None, inplace=True)\n",
    "ratings_csv.movieId.replace(to_replace=movie_ids_map, value=None, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "         ... \n",
       "100831    609\n",
       "100832    609\n",
       "100833    609\n",
       "100834    609\n",
       "100835    609\n",
       "Name: userId, Length: 100836, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv.userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>609</td>\n",
       "      <td>3120</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>609</td>\n",
       "      <td>2035</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>609</td>\n",
       "      <td>3121</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>609</td>\n",
       "      <td>1392</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>609</td>\n",
       "      <td>2873</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            0        0     4.0   964982703\n",
       "1            0        1     4.0   964981247\n",
       "2            0        2     4.0   964982224\n",
       "3            0        3     5.0   964983815\n",
       "4            0        4     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     609     3120     4.0  1493848402\n",
       "100832     609     2035     5.0  1493850091\n",
       "100833     609     3121     5.0  1494273047\n",
       "100834     609     1392     5.0  1493846352\n",
       "100835     609     2873     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_csv=ratings_csv.sample(frac=1) # shuffle les lignes du df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100337</th>\n",
       "      <td>609</td>\n",
       "      <td>6026</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1479542019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>44</td>\n",
       "      <td>1034</td>\n",
       "      <td>5.0</td>\n",
       "      <td>950726248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57111</th>\n",
       "      <td>379</td>\n",
       "      <td>109</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1494932491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14822</th>\n",
       "      <td>93</td>\n",
       "      <td>291</td>\n",
       "      <td>3.0</td>\n",
       "      <td>843406846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25569</th>\n",
       "      <td>176</td>\n",
       "      <td>5258</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1435837856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26560</th>\n",
       "      <td>181</td>\n",
       "      <td>193</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055151017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67116</th>\n",
       "      <td>433</td>\n",
       "      <td>322</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1270604764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31285</th>\n",
       "      <td>216</td>\n",
       "      <td>2573</td>\n",
       "      <td>2.0</td>\n",
       "      <td>955944922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58788</th>\n",
       "      <td>381</td>\n",
       "      <td>1312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1515161969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73735</th>\n",
       "      <td>473</td>\n",
       "      <td>8183</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1140467418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "100337     609     6026     3.5  1479542019\n",
       "6675        44     1034     5.0   950726248\n",
       "57111      379      109     3.0  1494932491\n",
       "14822       93      291     3.0   843406846\n",
       "25569      176     5258     2.5  1435837856\n",
       "...        ...      ...     ...         ...\n",
       "26560      181      193     2.5  1055151017\n",
       "67116      433      322     4.0  1270604764\n",
       "31285      216     2573     2.0   955944922\n",
       "58788      381     1312     4.0  1515161969\n",
       "73735      473     8183     3.5  1140467418\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_90 = ratings_csv.sample(frac = 0.9)\n",
    "\n",
    "rest_part_10 = ratings_csv.drop(part_90.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16921</th>\n",
       "      <td>104</td>\n",
       "      <td>4446</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1526207760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35057</th>\n",
       "      <td>233</td>\n",
       "      <td>172</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1004408188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55693</th>\n",
       "      <td>367</td>\n",
       "      <td>5435</td>\n",
       "      <td>3.0</td>\n",
       "      <td>975829315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38588</th>\n",
       "      <td>264</td>\n",
       "      <td>115</td>\n",
       "      <td>5.0</td>\n",
       "      <td>965315485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22556</th>\n",
       "      <td>152</td>\n",
       "      <td>946</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1525551652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37587</th>\n",
       "      <td>253</td>\n",
       "      <td>73</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1180446169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85179</th>\n",
       "      <td>552</td>\n",
       "      <td>8754</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1219556862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>18</td>\n",
       "      <td>381</td>\n",
       "      <td>2.0</td>\n",
       "      <td>965705917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87713</th>\n",
       "      <td>564</td>\n",
       "      <td>322</td>\n",
       "      <td>3.0</td>\n",
       "      <td>846533245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65282</th>\n",
       "      <td>417</td>\n",
       "      <td>2132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1461868255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90752 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating   timestamp\n",
       "16921     104     4446     4.5  1526207760\n",
       "35057     233      172     5.0  1004408188\n",
       "55693     367     5435     3.0   975829315\n",
       "38588     264      115     5.0   965315485\n",
       "22556     152      946     0.5  1525551652\n",
       "...       ...      ...     ...         ...\n",
       "37587     253       73     4.0  1180446169\n",
       "85179     552     8754     3.5  1219556862\n",
       "2629       18      381     2.0   965705917\n",
       "87713     564      322     3.0   846533245\n",
       "65282     417     2132     5.0  1461868255\n",
       "\n",
       "[90752 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14822</th>\n",
       "      <td>93</td>\n",
       "      <td>291</td>\n",
       "      <td>3.0</td>\n",
       "      <td>843406846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11892</th>\n",
       "      <td>72</td>\n",
       "      <td>905</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1464196245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86356</th>\n",
       "      <td>559</td>\n",
       "      <td>1226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1469650335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76856</th>\n",
       "      <td>479</td>\n",
       "      <td>2970</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1179161012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46743</th>\n",
       "      <td>306</td>\n",
       "      <td>497</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1186172544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96687</th>\n",
       "      <td>602</td>\n",
       "      <td>3293</td>\n",
       "      <td>2.0</td>\n",
       "      <td>963178568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38329</th>\n",
       "      <td>262</td>\n",
       "      <td>130</td>\n",
       "      <td>4.0</td>\n",
       "      <td>941590939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27911</th>\n",
       "      <td>189</td>\n",
       "      <td>2047</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1504310174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50046</th>\n",
       "      <td>321</td>\n",
       "      <td>192</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217676354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73735</th>\n",
       "      <td>473</td>\n",
       "      <td>8183</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1140467418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10084 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating   timestamp\n",
       "14822      93      291     3.0   843406846\n",
       "11892      72      905     4.5  1464196245\n",
       "86356     559     1226     4.0  1469650335\n",
       "76856     479     2970     4.0  1179161012\n",
       "46743     306      497     2.5  1186172544\n",
       "...       ...      ...     ...         ...\n",
       "96687     602     3293     2.0   963178568\n",
       "38329     262      130     4.0   941590939\n",
       "27911     189     2047     4.0  1504310174\n",
       "50046     321      192     4.0  1217676354\n",
       "73735     473     8183     3.5  1140467418\n",
       "\n",
       "[10084 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_part_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test_sets(data_path, train_prop = 0.9):\n",
    "    \"\"\"\n",
    "    Build train and test sets and reindex userIds and MovieIds from 0 with contiguous indexes.\n",
    "    \n",
    "    Input: \n",
    "        data_path : string : the path to the ratings file\n",
    "        train_prop : float : The proportion of the training set \n",
    "    \n",
    "    Output:\n",
    "        train : pandas.DataFrame : A dataframe with columns [userId, movieId, rating, timestamp], where\n",
    "            the userId and movieId value have been replaced with new ids starting at 0. \n",
    "            Contains `train_prop` random entries from the input file.\n",
    "        test : pandas.DataFrame : Same as `train`, contains the 1 - `train_prop` remaining entries.\n",
    "        nb_users : int : Number of unique user ids\n",
    "        nb_movies : int : Number of unique movie ids\n",
    "        user_ids_map : dict : A mapping of original file userId to a new index starting at 0.\n",
    "            Keys are int from the original userId column, values are int of the new indexation.\n",
    "        movie_ids_map : dict : Same as `user_ids_map` for the movieIds.\n",
    "    \"\"\"\n",
    "    # import du dataset dans un df\n",
    "    ratings=pd.read_csv(data_path)\n",
    "    \n",
    "    # on compte le nombre de users et de movies\n",
    "    nb_users=ratings.userId.nunique()\n",
    "    nb_movies=ratings.movieId.nunique()\n",
    "    \n",
    "    # création d'un dico pour remplacer les userId\n",
    "    user_ids_map={}\n",
    "    index=0\n",
    "    for user in ratings.userId.unique():\n",
    "        user_ids_map[user]=index\n",
    "        index+=1\n",
    "    user_ids_map\n",
    "    \n",
    "    # création d'un dico pour remplacer les movieId\n",
    "    movie_ids_map={}\n",
    "    index=0\n",
    "    for movie in ratings.movieId.unique():\n",
    "        movie_ids_map[movie]=index\n",
    "        index+=1\n",
    "    movie_ids_map\n",
    "    \n",
    "    # remplacement des userId et des movieId\n",
    "    ratings.userId.replace(to_replace=user_ids_map, value=None, inplace=True)\n",
    "    ratings.movieId.replace(to_replace=movie_ids_map, value=None, inplace=True)\n",
    "    \n",
    "    # on split les données en train (90%) et test set (10%) en les mélangeant au préalable avec sample\n",
    "    train=ratings.sample(frac = train_prop)\n",
    "    test=ratings.drop(train.index)\n",
    "\n",
    "    return train, test, nb_users, nb_movies, user_ids_map, movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9724 movies, 610 users, and 100836 ratings\n"
     ]
    }
   ],
   "source": [
    "# on applique la fonction qu'on vient de créer\n",
    "\n",
    "ratings_s_path =  '../data/ml-latest-small/ratings.csv'\n",
    "train, test, nb_users, nb_movies, user_ids_map, movie_ids_map = get_train_test_sets(ratings_s_path)\n",
    "dataset = pd.concat((train,test), axis = 0)\n",
    "\n",
    "print(\"There are %i movies, %i users, and %i ratings\" % (nb_movies, nb_users, dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fait les split entre X et y et on convertit tout en numpy array\n",
    "\n",
    "X_train = [train[\"userId\"].to_numpy(), train[\"movieId\"].to_numpy()]\n",
    "y_train = train[\"rating\"].to_numpy()\n",
    "\n",
    "X_test = [test[\"userId\"].to_numpy(), test[\"movieId\"].to_numpy()]\n",
    "y_test = test[\"rating\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the ratings distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on visualise la répartition des notes des movies par les users\n",
    "\n",
    "dataset['rating'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset there are a lot of missing values, because not all the user/movie pairs have an associated rating. Indeed, each user rates only a few movies ! The goal of this notebook is to predict (some of) the missing user/movie ratings.\n",
    "\n",
    "Print how many movies each of the 5 first users have rated, and print the percentage of available ratings in the whole dataset (i.e. the ratio between number of ratings and all the possible users/movies combinations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69094    447\n",
       "92212    596\n",
       "5566      39\n",
       "37828    255\n",
       "94055    598\n",
       "Name: userId, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOFILL\n",
    "\n",
    "# Nombre de movies rated par les premiers users (une ligne par évaluation)\n",
    "\n",
    "dataset.userId.head() # donne les 5 first users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447 1864\n",
      "596 443\n",
      "39 103\n",
      "255 174\n",
      "598 2478\n"
     ]
    }
   ],
   "source": [
    "for user in dataset.userId.head():\n",
    "    print(user,dataset[dataset.userId == user].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pourcentage d'évaluation non-faites sur tout le dataset\n",
    "\n",
    "dataset.shape[0] # nombre de ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb_users: 610\n",
      "Nb_movies: 9724\n",
      "Combinaisons possibles: 5931640\n"
     ]
    }
   ],
   "source": [
    "# toutes les combinaisons possibles user/movie\n",
    "\n",
    "print(\"Nb_users:\",nb_users)\n",
    "print(\"Nb_movies:\",nb_movies)\n",
    "print(\"Combinaisons possibles:\",nb_users*nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage d'evaluations realisees: 1.6999683055613624\n"
     ]
    }
   ],
   "source": [
    "pourcentage_ratings_available=(dataset.shape[0])/(nb_users*nb_movies)*100\n",
    "print(\"Pourcentage d'evaluations realisees:\",pourcentage_ratings_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 1.7% of ratings that are available, which is normal as each hasn't rated all the movies. To see the dataset in a matrix form with all the missing ratings, use the `Dataframe.pivot()` function, with the `userId` as index, the `movieId` as columns, and the ratings for the `values` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9714</th>\n",
       "      <th>9715</th>\n",
       "      <th>9716</th>\n",
       "      <th>9717</th>\n",
       "      <th>9718</th>\n",
       "      <th>9719</th>\n",
       "      <th>9720</th>\n",
       "      <th>9721</th>\n",
       "      <th>9722</th>\n",
       "      <th>9723</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "userId                                                               ...   \n",
       "0         4.0   4.0   4.0   5.0   5.0   3.0   5.0   4.0   5.0   5.0  ...   \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3         NaN   NaN   NaN   2.0   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4         4.0   NaN   NaN   NaN   4.0   NaN   NaN   4.0   NaN   NaN  ...   \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "605       2.5   NaN   NaN   3.0   4.5   4.0   NaN   3.5   NaN   4.0  ...   \n",
       "606       4.0   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN   NaN  ...   \n",
       "607       2.5   2.0   NaN   4.5   4.5   3.0   NaN   4.0   NaN   NaN  ...   \n",
       "608       3.0   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN   NaN  ...   \n",
       "609       5.0   NaN   5.0   5.0   4.0   4.0   NaN   4.5   NaN   NaN  ...   \n",
       "\n",
       "movieId  9714  9715  9716  9717  9718  9719  9720  9721  9722  9723  \n",
       "userId                                                               \n",
       "0         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "605       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "606       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "607       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "608       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "609       3.0   3.5   3.5   3.5   3.5   2.5   4.5   3.0   3.5   3.5  \n",
       "\n",
       "[610 rows x 9724 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOFILL\n",
    "dataset.pivot(index='userId',columns='movieId',values='rating')\n",
    "#  représentation de la matrice - en pratique on s'en sert jamais car la matrice sera trop grande, elle ne \n",
    "#  rentrera pas dans la mémoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the ratings of user 1. To do so, use the *movies.csv* file and your `movie_ids_map` dictionnary to find the movie title from the new movie indexes, and print the real movie title associated to each rating of user 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First user: 447.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69094</th>\n",
       "      <td>447</td>\n",
       "      <td>5064</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019127916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68716</th>\n",
       "      <td>447</td>\n",
       "      <td>2166</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019137647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68944</th>\n",
       "      <td>447</td>\n",
       "      <td>2998</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1161526179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69662</th>\n",
       "      <td>447</td>\n",
       "      <td>1238</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1149412972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69103</th>\n",
       "      <td>447</td>\n",
       "      <td>1653</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1062677841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70480</th>\n",
       "      <td>447</td>\n",
       "      <td>8040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1481041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70481</th>\n",
       "      <td>447</td>\n",
       "      <td>8041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1492796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70487</th>\n",
       "      <td>447</td>\n",
       "      <td>8045</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1476469445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70490</th>\n",
       "      <td>447</td>\n",
       "      <td>4606</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1487534510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70499</th>\n",
       "      <td>447</td>\n",
       "      <td>8052</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1470510722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1864 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating   timestamp\n",
       "69094     447     5064     3.0  1019127916\n",
       "68716     447     2166     3.0  1019137647\n",
       "68944     447     2998     1.5  1161526179\n",
       "69662     447     1238     3.0  1149412972\n",
       "69103     447     1653     3.5  1062677841\n",
       "...       ...      ...     ...         ...\n",
       "70480     447     8040     2.0  1481041186\n",
       "70481     447     8041     0.5  1492796600\n",
       "70487     447     8045     1.5  1476469445\n",
       "70490     447     4606     1.5  1487534510\n",
       "70499     447     8052     1.5  1470510722\n",
       "\n",
       "[1864 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOFILL\n",
    "\n",
    "# on récupère les movieId de user 1\n",
    "\n",
    "print(\"First user:\",dataset.iloc[0][0])\n",
    "df_ratings_user1=dataset[dataset.userId == dataset.iloc[0][0]]\n",
    "df_ratings_user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d5learner-05/anaconda3/envs/keras_gpu/lib/python3.8/site-packages/pandas/core/series.py:4563: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69094</th>\n",
       "      <td>447</td>\n",
       "      <td>2779</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019127916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68716</th>\n",
       "      <td>447</td>\n",
       "      <td>428</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019137647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68944</th>\n",
       "      <td>447</td>\n",
       "      <td>1982</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1161526179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69662</th>\n",
       "      <td>447</td>\n",
       "      <td>33004</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1149412972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69103</th>\n",
       "      <td>447</td>\n",
       "      <td>2806</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1062677841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70480</th>\n",
       "      <td>447</td>\n",
       "      <td>157130</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1481041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70481</th>\n",
       "      <td>447</td>\n",
       "      <td>157172</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1492796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70487</th>\n",
       "      <td>447</td>\n",
       "      <td>157407</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1476469445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70490</th>\n",
       "      <td>447</td>\n",
       "      <td>158813</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1487534510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70499</th>\n",
       "      <td>447</td>\n",
       "      <td>160440</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1470510722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1864 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating   timestamp\n",
       "69094     447     2779     3.0  1019127916\n",
       "68716     447      428     3.0  1019137647\n",
       "68944     447     1982     1.5  1161526179\n",
       "69662     447    33004     3.0  1149412972\n",
       "69103     447     2806     3.5  1062677841\n",
       "...       ...      ...     ...         ...\n",
       "70480     447   157130     2.0  1481041186\n",
       "70481     447   157172     0.5  1492796600\n",
       "70487     447   157407     1.5  1476469445\n",
       "70490     447   158813     1.5  1487534510\n",
       "70499     447   160440     1.5  1470510722\n",
       "\n",
       "[1864 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on regarde l'index original de chaque movie\n",
    "\n",
    "inv_movie_ids_map = {v: k for k, v in movie_ids_map.items()} # on inverse le dico\n",
    "df_ratings_user1.movieId.replace(to_replace=inv_movie_ids_map, value=None, inplace=True)\n",
    "df_ratings_user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                      title  \\\n",
       "0           1                           Toy Story (1995)   \n",
       "1           2                             Jumanji (1995)   \n",
       "2           3                    Grumpier Old Men (1995)   \n",
       "3           4                   Waiting to Exhale (1995)   \n",
       "4           5         Father of the Bride Part II (1995)   \n",
       "...       ...                                        ...   \n",
       "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "9738   193583               No Game No Life: Zero (2017)   \n",
       "9739   193585                               Flint (2017)   \n",
       "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                           genres  \n",
       "0     Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                      Adventure|Children|Fantasy  \n",
       "2                                  Comedy|Romance  \n",
       "3                            Comedy|Drama|Romance  \n",
       "4                                          Comedy  \n",
       "...                                           ...  \n",
       "9737              Action|Animation|Comedy|Fantasy  \n",
       "9738                     Animation|Comedy|Fantasy  \n",
       "9739                                        Drama  \n",
       "9740                             Action|Animation  \n",
       "9741                                       Comedy  \n",
       "\n",
       "[9742 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on récupère le nom des movies associés à l'index\n",
    "movies=pd.read_csv('../data/ml-latest-small/movies.csv')\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>447</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019127916</td>\n",
       "      <td>3717.0</td>\n",
       "      <td>Gone in 60 Seconds (2000)</td>\n",
       "      <td>Action|Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>447</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019137647</td>\n",
       "      <td>491.0</td>\n",
       "      <td>Man Without a Face, The (1993)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>447</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1161526179</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>Saragossa Manuscript, The (Rekopis znaleziony ...</td>\n",
       "      <td>Adventure|Drama|Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33004</th>\n",
       "      <td>447</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1149412972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>447</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1062677841</td>\n",
       "      <td>3751.0</td>\n",
       "      <td>Chicken Run (2000)</td>\n",
       "      <td>Animation|Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157130</th>\n",
       "      <td>447</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1481041186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157172</th>\n",
       "      <td>447</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1492796600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157407</th>\n",
       "      <td>447</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1476469445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158813</th>\n",
       "      <td>447</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1487534510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160440</th>\n",
       "      <td>447</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1470510722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1864 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  rating   timestamp  movieId  \\\n",
       "movieId                                        \n",
       "2779        447     3.0  1019127916   3717.0   \n",
       "428         447     3.0  1019137647    491.0   \n",
       "1982        447     1.5  1161526179   2632.0   \n",
       "33004       447     3.0  1149412972      NaN   \n",
       "2806        447     3.5  1062677841   3751.0   \n",
       "...         ...     ...         ...      ...   \n",
       "157130      447     2.0  1481041186      NaN   \n",
       "157172      447     0.5  1492796600      NaN   \n",
       "157407      447     1.5  1476469445      NaN   \n",
       "158813      447     1.5  1487534510      NaN   \n",
       "160440      447     1.5  1470510722      NaN   \n",
       "\n",
       "                                                     title  \\\n",
       "movieId                                                      \n",
       "2779                             Gone in 60 Seconds (2000)   \n",
       "428                         Man Without a Face, The (1993)   \n",
       "1982     Saragossa Manuscript, The (Rekopis znaleziony ...   \n",
       "33004                                                  NaN   \n",
       "2806                                    Chicken Run (2000)   \n",
       "...                                                    ...   \n",
       "157130                                                 NaN   \n",
       "157172                                                 NaN   \n",
       "157407                                                 NaN   \n",
       "158813                                                 NaN   \n",
       "160440                                                 NaN   \n",
       "\n",
       "                            genres  \n",
       "movieId                             \n",
       "2779                  Action|Crime  \n",
       "428                          Drama  \n",
       "1982       Adventure|Drama|Mystery  \n",
       "33004                          NaN  \n",
       "2806     Animation|Children|Comedy  \n",
       "...                            ...  \n",
       "157130                         NaN  \n",
       "157172                         NaN  \n",
       "157407                         NaN  \n",
       "158813                         NaN  \n",
       "160440                         NaN  \n",
       "\n",
       "[1864 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_user1=df_ratings_user1.set_index('movieId').join(movies,on='movieId') # on join les 2 df grâce au movieId\n",
    "joined_user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId\n",
      "2779                              Gone in 60 Seconds (2000)\n",
      "428                          Man Without a Face, The (1993)\n",
      "1982      Saragossa Manuscript, The (Rekopis znaleziony ...\n",
      "33004                                                   NaN\n",
      "2806                                     Chicken Run (2000)\n",
      "                                ...                        \n",
      "157130                                                  NaN\n",
      "157172                                                  NaN\n",
      "157407                                                  NaN\n",
      "158813                                                  NaN\n",
      "160440                                                  NaN\n",
      "Name: title, Length: 1864, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(joined_user1.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a python library made for easily designing complex models such as deep learning models, in this module we are going to use just a few features from it to implement our simple matrix factorization model, as it makes a good introduction to the library before the next module about deep learning where you will also be using Keras.\n",
    "\n",
    "The following function `get_mf_model` implements the model described in equation (2) in Koren's paper (without the $+\\lambda(\\ldots)$ part for the moment). So it basically tries to find the $p_u \\in \\mathbb{R}^k$ and $q_i \\in \\mathbb{R}^k$ vectors that minimizes the squared loss between their dot product $p_u^Tq_i$, and the observed ratings $r_{ui}$, from random initialization of $p_u$ and $q_i$. In machine learning terms, $p_u$ and $q_i$ are called the *embeddings* of the user $u$ and of the movie $i$ respectively. Their size $k$ is an hyper-parameter of the model, which is called the *rank* of the factorization.\n",
    "\n",
    "To do so, it uses the functional API from Keras (the other API proposed is the sequential one, but is not adapted for this model), you can read about it here : https://keras.io/getting-started/functional-api-guide/ .\n",
    "\n",
    "Keras, unlike Numpy, uses a different progamming paradigm. Numpy uses an *imperative* programming style (like python in general), meaning that when you execute `x.dot(y)`, the dot product is actually calculated. Keras however, uses a *declarative* (also called *symbolic*) programming style, meaning that when you write `Dot()([x, y])`, you tell Keras than when you will call the *fit* function of your model in the future, you will want to do a dot product between the future values that *x* and *y* will have. And this is what Keras is about, it allows you to build your own model as a sequence of operations, describing each input and output, and then later fit it and predict with it.\n",
    "\n",
    "Let's not get in too many details, but retain that the `get_mf_model` function below is not actually executing the model, it creates it, and returns an object of the class `keras.models.Model` that has been instructed with your model operations, and this object can then be trained with the classic `fit` and `predict` functions. \n",
    "\n",
    "Read carefully the comments in the code of the function to understand the different steps in the model creation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape\n",
    "from keras.layers.merge import Dot\n",
    "\n",
    "def get_mf_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a simple matrix factorization model from\n",
    "    the number of user, the number of movies, and the size of the embeddings k.\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization\n",
    "        \n",
    "    \"\"\"\n",
    "    dim_embedddings = k\n",
    "    \n",
    "    #Inputs:\n",
    "    #First we describe the input of the model, that is the training data that we will give it as X\n",
    "    #In our case, the input are just the user index u and the movie index i.\n",
    "    #So we declare two inputs of size one:\n",
    "    u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    #Then let's declare our variable, the embeddings p and q.\n",
    "    #First with the users, we declare that we have nb_users embeddings, each of size dim_embeddings.\n",
    "    #An embedding object is indexed by calling it with the index parameter like a function,\n",
    "    #so we add a `(u)` at the end to tell keras we want it to be indexed \n",
    "    #by the user ids we will pass at training time as inputs.\n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    \n",
    "    #Unfortunatly, when indexing an embeddings it keeps [1,k] matrix shape instead\n",
    "    #of just a [k] vector, so we have to tell Keras that we just want a vector by\n",
    "    #redefining its shape:\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Same thing for the movie embeddings:\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    #Then the dot product between the two indexed embeddings, \n",
    "    #we'll understand the axes = 1 part later.\n",
    "    r_hat = Dot(axes = 1)([q_i, p_u])\n",
    "\n",
    "    #We define our model by giving its input and outputs, in our case\n",
    "    #the user and movie ids will be the inputs, and the output will be\n",
    "    #the estimated rating r_hat, that is the dot product of the \n",
    "    #corresponding embeddings.\n",
    "    model = Model(inputs=[u, i], outputs=r_hat)\n",
    "    \n",
    "    #Finally, we define the loss and metric to use, in our case the mean squared error,\n",
    "    #along with the optimization method, we'll understand what is 'adam' later also.\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "mf_model = get_mf_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows us to have a textual overview of the model we defined with the *summary()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "i__movie_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "u__user_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q_i__movie_embedding (Embedding (None, 1, 30)        291720      i__movie_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_u__user_embedding (Embedding) (None, 1, 30)        18300       u__user_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "q_i__movie_embedding_reshaped ( (None, 30)           0           q_i__movie_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "p_u__user_embedding_reshaped (R (None, 30)           0           p_u__user_embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           q_i__movie_embedding_reshaped[0][\n",
      "                                                                 p_u__user_embedding_reshaped[0][0\n",
      "==================================================================================================\n",
      "Total params: 310,020\n",
      "Trainable params: 310,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the keras objects we defined in our model is called a *layer*, and we find them in order in the first column. The *Param #* column gives the number of trainable parameters of the layer, in our case these are just the embeddings, and they should be equal to $nb\\_users \\times k$ and $nb\\_movies \\times k$. The *Connected to* column tells for each layer which layers are inputs for this layer (you can safely ignore the `[0][0]` for this module).\n",
    "\n",
    "Finally the *Output Shape* column gives us the shape of the layer, each layer being a *tensor*. A tensor is the generalization of matrices to more than two dimensions. So a matrix is a 2D-tensor and a vector is a 1D-tensor, and each layer can be a matrix, a vector, or a higher-order tensor. The output shape we see is indeed the expected one at each layer, except there is this `None` in first dimension, why is that ?\n",
    "\n",
    "To understand it, we have to get into how Keras is actually minimizing the mean squared loss of our model. In general, when in comes to minimizing error functions on big datasets, a generic method is to use Stocastic Gradient Descent (SGD), briefly described in page 4 of Koren's article. \n",
    "\n",
    "Read about gradient descent, SGD and its variant mini-batch SGD in Chapter 4 of *Hands on ML ...* (pages 111-120):\n",
    "https://www.lpsm.paris/pageperso/has/source/Hand-on-ML.pdf\n",
    "\n",
    "This is what Keras does when it fits the model, it initializes the $q_i$ and $p_u$ embedding vectors randomly, and then perform mini-batch SGD to find the minimum mean squared error on the training set. Since mini-batching means considering multiple training samples at the same time, Keras keeps the first dimension of each layer to stack the samples of each batch, this is why `None` is written, the actual batch_size being set at training time when calling the `fit` function. This is also why we had to set `axes=1` when calling the `Dot` layer in the `get_mf_model` function, because the first dimension (axe 0) of each layer is kept for the batches. And about the `optimizer='adam'`, it is just a variation of mini-batch SGD that is faster, we'll get into more details about SGD variations in the optional parts of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train our matrix factorization model on our movieLens data. The `epochs` parameter controls the number of iterations of the SGD algorithm, that is the number of times it is going to pass on each training rating and update the embeddings accordingly. Let's keep it at 20 for the moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 3s 4ms/step - loss: 13.3518 - mse: 13.3518\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 12.4153 - mse: 12.4153\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 6.4912 - mse: 6.4912\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 2.7561 - mse: 2.7561\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.8032 - mse: 1.8032\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.3854 - mse: 1.3854\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.1460 - mse: 1.1460\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.9988 - mse: 0.9988\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.8969 - mse: 0.8969\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.8232 - mse: 0.8232\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.7825 - mse: 0.7825\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.7332 - mse: 0.7332\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.7109 - mse: 0.7109\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6735 - mse: 0.6735\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6532 - mse: 0.6532\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6278 - mse: 0.6278\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6112 - mse: 0.6112\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6001 - mse: 0.6001\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5848 - mse: 0.5848\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5763 - mse: 0.5763\n"
     ]
    }
   ],
   "source": [
    "history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now try to predict the test ratings, and report our root mean squared error like in other regression problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test RMSE : 1.0736397023683735 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "y_pred = mf_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 1.1/1.2 RMSE, not that bad without any information about the users nor the movies ! This is the power of collaborative filtering models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the same model on your GPU and on your CPU, and compare the training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses the `tensorflow` python library for the computation part, if you have installed your GPU drivers and the GPU version of tensorflow, then it will run on your GPU by default. To get an idea of the computation speed-up given by GPUs, let's force keras to use the cpu instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 13.3497 - mse: 13.3497\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 12.3979 - mse: 12.3979\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 6.3586 - mse: 6.3586\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 2.7530 - mse: 2.7530\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 1.7990 - mse: 1.7990\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 1.3825 - mse: 1.3825\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1491 - mse: 1.1491\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 1.0011 - mse: 1.0011\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.8961 - mse: 0.8961\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.8243 - mse: 0.8243\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.7738 - mse: 0.7738\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.7330 - mse: 0.7330\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.7001 - mse: 0.7001\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6781 - mse: 0.6781\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6530 - mse: 0.6530\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6319 - mse: 0.6319\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6173 - mse: 0.6173\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6088 - mse: 0.6088\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5958 - mse: 0.5958\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5794 - mse: 0.5794\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    mf_model = get_mf_model(nb_users, nb_movies, k)\n",
    "    history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)\n",
    "\n",
    "# semble prendre moins de temps en utilisant le CPU au lieu du GPU (normalement c'est l'inverse, mais c'est\n",
    "# parce que notre modèle est très simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding user and movie bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enhance our matrix factorization model and add the user and movie biases to the rating estimation function as in equation (4) of Koren's paper ; except we will for the moment forget about the global bias $\\mu$ as it is not so intuitive to implement in Keras. Fill the function below to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import Add\n",
    "\n",
    "def get_mf_bias_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a smatrix factorization model with user and movie biases\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization with biases\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # on crée les matrices (variables) pour lesquelles on cherche les paramètres.\n",
    "    # on appelle des classes d'objets et on les instancie\n",
    "    \n",
    "    # chaque classe est une couche successive du réseau\n",
    "    \n",
    "    # Input : donne la taille des input du modèle. C'est les premières couches.\n",
    "    # Embedding : crée une matrice embedding de la bonne taille. (u) à la fin pour dire que dans cette matrice,\n",
    "    #             on va juste prendre des vecteur u.\n",
    "    # Reshape : le vecteur u est de taille (k,1). Donc Keras pense qu'il est en 2D, et que c'est donc une \n",
    "    #           matrice. Alors que mathématiquement, non. On supprime juste le 1 pour lui faire comprendre que\n",
    "    #           c'est un vecteur (k,).\n",
    "    # Dot : on fera le produit scalaire entre un vecteur p_u et un q_i pour faire des prédictions.\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    # Dot product\n",
    "    d = Dot(axes = 1)([p_u, q_i])\n",
    "    \n",
    "    #TOFILL\n",
    "    dim_bias = 1\n",
    "    \n",
    "    b_u = Embedding(nb_users,dim_bias,name='b_u_bias')(u)\n",
    "    b_u = Reshape((dim_bias,),name='b_u_reshaped')(b_u)\n",
    "    \n",
    "    b_i = Embedding(nb_movies,dim_bias,name='b_i_bias')(i)\n",
    "    b_i = Reshape((dim_bias,),name='b_i_reshaped')(b_i)\n",
    "        \n",
    "    d = Add()([d,b_u,b_i])\n",
    "\n",
    "    model = Model(inputs=[u, i], outputs=d)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Ici on fait de l'API fonctionnel. On a plusieurs données d'input (u et i), et on les transforme pour\n",
    "# finalement obtenir un output\n",
    "\n",
    "# En API séquentiel, on a qu'une seule input et output, donc pas besoin de ce chemin qu'on vient de créer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_bias_model = get_mf_bias_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "u__user_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "i__movie_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "p_u__user_embedding (Embedding) (None, 1, 30)        18300       u__user_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "q_i__movie_embedding (Embedding (None, 1, 30)        291720      i__movie_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_u__user_embedding_reshaped (R (None, 30)           0           p_u__user_embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "q_i__movie_embedding_reshaped ( (None, 30)           0           q_i__movie_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "b_u_bias (Embedding)            (None, 1, 1)         610         u__user_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b_i_bias (Embedding)            (None, 1, 1)         9724        i__movie_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           p_u__user_embedding_reshaped[0][0\n",
      "                                                                 q_i__movie_embedding_reshaped[0][\n",
      "__________________________________________________________________________________________________\n",
      "b_u_reshaped (Reshape)          (None, 1)            0           b_u_bias[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "b_i_reshaped (Reshape)          (None, 1)            0           b_i_bias[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           dot_2[0][0]                      \n",
      "                                                                 b_u_reshaped[0][0]               \n",
      "                                                                 b_i_reshaped[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 320,354\n",
      "Trainable params: 320,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_bias_model.summary()\n",
    "# les None c'est les dimensions réservées au batch (pour l'instant il ne les connait pas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 2s 4ms/step - loss: 12.9613 - mse: 12.9613\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 10.9402 - mse: 10.9402\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 5.3996 - mse: 5.3996\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 2.2941 - mse: 2.2941\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.5209 - mse: 1.5209\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.1910 - mse: 1.1910\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.9880 - mse: 0.9880\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.8721 - mse: 0.8721\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.7802 - mse: 0.7802\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.7371 - mse: 0.7371\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6889 - mse: 0.6889\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6518 - mse: 0.6518\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6187 - mse: 0.6187\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5973 - mse: 0.5973\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5828 - mse: 0.5828\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5622 - mse: 0.5622\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5415 - mse: 0.5415\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5325 - mse: 0.5325\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5124 - mse: 0.5124\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5031 - mse: 0.5031\n"
     ]
    }
   ],
   "source": [
    "history = mf_bias_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test RMSE : 1.0068784508512645 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)\n",
    "\n",
    "# on était à un MSE de 1.07 sans inclure les biais, et là, on descend à 1 : légère diminution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a lower RMSE, about 1.0/1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment we have omitted the regularization of the embeddings and bias parameters, as described in equation (5) of Koren's paper. We are now going to add them to the model, have a look at https://keras.io/layers/embeddings/ and https://keras.io/regularizers/ to see how to do this with keras. Fill the function below to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_):\n",
    "    \"\"\"\n",
    "    Build a smatrix factorization model with user and movie biases, and L2 regularization\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization with biases\n",
    "            and L2 regularization\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #TOFILL\n",
    "    dim_embedddings = k\n",
    "\n",
    "    # pour que les valeurs dans p et q soient trop grandes -> ca veut dire qu'elles overfit probablement\n",
    "    # pour pas que certaines variables dans les embeddings prennnent trop d'importance, on applique des poids\n",
    "    # au embeddings (comme dans la regression lineaire). Ca permet d'éviter l'overfitting. En faisant ca, on a\n",
    "    # un nouvel hyperparamètre à définir, lambda, de sorte qu'il soit pas trop petit pour pas que certaines\n",
    "    # variables prennent le dessus sur les prédictions, et pas trop grand pour que toutes les variables n'aient\n",
    "    # pas forcément le même impact.\n",
    "\n",
    "    # User embeddings + regularizers\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    p_u = Embedding(nb_users, dim_embedddings, embeddings_regularizer=regularizers.l2(lambda_), name=\"p_u__user_embedding\")(u)\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "\n",
    "\n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, embeddings_regularizer=regularizers.l2(lambda_), name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "\n",
    "    # Dot product\n",
    "    d = Dot(axes = 1)([p_u, q_i])\n",
    "    \n",
    "    # Bias + regularizers\n",
    "    dim_bias = 1\n",
    "\n",
    "    b_u = Embedding(nb_users,dim_bias,embeddings_regularizer=regularizers.l2(lambda_),name='b_u_bias')(u)\n",
    "    b_u = Reshape((dim_bias,),name='b_u_reshaped')(b_u)\n",
    "\n",
    "    b_i = Embedding(nb_movies,dim_bias,embeddings_regularizer=regularizers.l2(lambda_),name='b_i_bias')(i)\n",
    "    b_i = Reshape((dim_bias,),name='b_i_reshaped')(b_i)\n",
    "    \n",
    "    d = Add()([d,b_u,b_i])\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[u, i], outputs=d)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.00001\n",
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "u__user_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "i__movie_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "p_u__user_embedding (Embedding) (None, 1, 30)        18300       u__user_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "q_i__movie_embedding (Embedding (None, 1, 30)        291720      i__movie_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_u__user_embedding_reshaped (R (None, 30)           0           p_u__user_embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "q_i__movie_embedding_reshaped ( (None, 30)           0           q_i__movie_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "b_u_bias (Embedding)            (None, 1, 1)         610         u__user_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b_i_bias (Embedding)            (None, 1, 1)         9724        i__movie_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1)            0           p_u__user_embedding_reshaped[0][0\n",
      "                                                                 q_i__movie_embedding_reshaped[0][\n",
      "__________________________________________________________________________________________________\n",
      "b_u_reshaped (Reshape)          (None, 1)            0           b_u_bias[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "b_i_reshaped (Reshape)          (None, 1)            0           b_i_bias[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dot_6[0][0]                      \n",
      "                                                                 b_u_reshaped[0][0]               \n",
      "                                                                 b_i_reshaped[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 320,354\n",
      "Trainable params: 320,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_bias_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6575 - mse: 0.5198\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6316 - mse: 0.4935\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6139 - mse: 0.4753\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5993 - mse: 0.4601\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5866 - mse: 0.4470\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5740 - mse: 0.4340\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5630 - mse: 0.4225\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5518 - mse: 0.4109\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5416 - mse: 0.4002\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5308 - mse: 0.3890\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5209 - mse: 0.3787\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5111 - mse: 0.3685\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5014 - mse: 0.3584\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4916 - mse: 0.3481\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4822 - mse: 0.3383\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4730 - mse: 0.3288\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4639 - mse: 0.3191\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4551 - mse: 0.3099\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4462 - mse: 0.3006\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4378 - mse: 0.2918\n"
     ]
    }
   ],
   "source": [
    "history = mf_bias_reg_model.fit(X_train, y_train, epochs=20, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29183778166770935"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['mse'][-1] # récupère le score du dernier MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test RMSE : 1.0251459257619049 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_reg_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)\n",
    "\n",
    "# légèrement moins bon, mais normalement doit être un peu plus petit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a slightly better RMSE, but sometimes regularization is very important for achieving good test performances, in depends on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of setting manually the maximum number of epochs, we prefer to use *early stopping*. When training with early stopping, keras keeps a given validation set though the parameter `validation_split`, on which it is going to monitor a performance measure you give it (here the `mse`) at every epoch, and continue optimization while the mse on the validation set keeps going down, and stops it when it goes back up. This mechanism is an easy way to avoid over-fitting, you can read more about it there : https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "In general when using early stopping we setup a high number of maximum epochs, that is never reach because the optimization is stopped by early stopping first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 8ms/step - loss: 13.0772 - mse: 13.0753 - val_loss: 11.9659 - val_mse: 11.9640\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 11.3107 - mse: 11.3074 - val_loss: 7.9165 - val_mse: 7.9034\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 6.4495 - mse: 6.4304 - val_loss: 3.4660 - val_mse: 3.4272\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.8384 - mse: 2.7941 - val_loss: 2.2258 - val_mse: 2.1673\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8006 - mse: 1.7384 - val_loss: 1.7895 - val_mse: 1.7176\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.4042 - mse: 1.3295 - val_loss: 1.5688 - val_mse: 1.4865\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1798 - mse: 1.0952 - val_loss: 1.4424 - val_mse: 1.3517\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0405 - mse: 0.9479 - val_loss: 1.3625 - val_mse: 1.2646\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9390 - mse: 0.8396 - val_loss: 1.3109 - val_mse: 1.2070\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8837 - mse: 0.7785 - val_loss: 1.2751 - val_mse: 1.1660\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8311 - mse: 0.7209 - val_loss: 1.2518 - val_mse: 1.1383\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7965 - mse: 0.6821 - val_loss: 1.2360 - val_mse: 1.1189\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7703 - mse: 0.6522 - val_loss: 1.2251 - val_mse: 1.1047\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7356 - mse: 0.6145 - val_loss: 1.2183 - val_mse: 1.0951\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7139 - mse: 0.5901 - val_loss: 1.2128 - val_mse: 1.0872\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6976 - mse: 0.5715 - val_loss: 1.2098 - val_mse: 1.0821\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6868 - mse: 0.5586 - val_loss: 1.2091 - val_mse: 1.0796\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6706 - mse: 0.5407 - val_loss: 1.2085 - val_mse: 1.0774\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6614 - mse: 0.5301 - val_loss: 1.2092 - val_mse: 1.0768\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6483 - mse: 0.5157 - val_loss: 1.2092 - val_mse: 1.0756\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6332 - mse: 0.4995 - val_loss: 1.2108 - val_mse: 1.0763\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6277 - mse: 0.4930 - val_loss: 1.2120 - val_mse: 1.0767\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6139 - mse: 0.4784 - val_loss: 1.2150 - val_mse: 1.0789\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6068 - mse: 0.4705 - val_loss: 1.2177 - val_mse: 1.0809\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.5991 - mse: 0.4623 - val_loss: 1.2198 - val_mse: 1.0823\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cfe00c0d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A problem with training neural networks is in the choice of the number of training epochs to use. Too many \n",
    "# epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model.\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=5, verbose=1)\n",
    "# monitor : performance measure to monitor in order to end training\n",
    "# by default, mode is set to ‘auto‘ and knows that you want to minimize loss or maximize accuracy.\n",
    "# verbose : to discover the training epoch on which training was stopped\n",
    "# patience : number of epochs on which we would like to see no improvement before stopping to train\n",
    "mf_bias_reg_model.fit(X_train, y_train, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# il s'arrête à epoch+5 car patience de 5, mais du coup il récupère pas les meilleurs paramètres, seulement\n",
    "# la dernière. Si on veut la meilleure, faut récupérer la 5e avant la fin\n",
    "# sinon on peut récup les best params avec un paramètre du early_stopping (par défaut False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse (train set) < val_mse (validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training stops before 500 epochs, when the validation MSE stops decreasing during 5 consecutive epochs (the patience value = 5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search embedding size and regularization factor with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the moment we didn't grid search our model hyper-parameters, such as `k` and `lambda_`. There exists some scikit-learn wrappers for keras models in order to use scikit grid search functions, unfortunately they only work with single input keras models, which is not our case as we have two inputs: the user and the movie indexes.\n",
    "\n",
    "So let's implement your own grid search function for the two parameters `k` and `lambda_`. With big enough datasets, it is not necessary to do a cross-validation for each hyper-parameter combination, and we can simply split the training set into a sub-training set and a validation set to test our hyper-parameters. It does work because the validation set is big enough to see enough data variations, and with very big datasets, it is anyway not possible anymore to do a full cross-validation as it takes too much time to train. \n",
    "\n",
    "Fill in the `grid_search` function below that splits the training set 90% train / 10% validation, then train (with early stopping) and compute the validation RMSE for all the hyper-parameter combinations from the `param_grid` dictionary of hyper-parameter values, with the model returned by the `get_model_function` (yes, you can pass functions as parameters!), and return the hyper-parameters that give the lowest RMSE on the 10% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def grid_search(data, param_grid, get_model_function, nb_users, nb_movies, validation_size = 0.1):\n",
    "    \"\"\"\n",
    "    Performs a grid search over the \n",
    "    \n",
    "    Input:\n",
    "        data : DataFrame : The training set to be split between training and validation sets\n",
    "        param_grid : dict : Dictionary containing the values of the hyper-parameters to grid-search\n",
    "        get_model_function : function : A function that returns the keras model to grid-search\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        validation_size : float : Proportion of the validation set\n",
    "        \n",
    "    Output:\n",
    "        best_params : dict : A dictionary of the best hyper-parameters values\n",
    "        best_score : float : The validation RMSE corresponding to the best\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "    \n",
    "    #TOFILL\n",
    "    \n",
    "    # Split training dataset en X_train et y_train (le split validation se fera au moment du model.fit())\n",
    "    \n",
    "    X_train = [data[\"userId\"].to_numpy(), data[\"movieId\"].to_numpy()]\n",
    "    y_train = data[\"rating\"].to_numpy()\n",
    "    \n",
    "    # On crée une liste avec toutes les combinaisons d'hyperparamètres\n",
    "    \n",
    "    combinations = itertools.product(*param_grid.values())\n",
    "    \n",
    "    # On entraine le modèle (avec early_stopping) sur chaque combinaison et on calcule le mse\n",
    "        \n",
    "    for combinaison in combinations:\n",
    "        model = get_model_function(nb_users, nb_movies, *combinaison)\n",
    "        early_stopping = EarlyStopping(monitor='val_mse', patience=5, verbose=1)\n",
    "        history=model.fit(X_train, y_train, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Si le MSE est inférieur aux précédents, il est stocké avec les hyperparamètres correspondants\n",
    "    \n",
    "        if history.history['val_mse'][-1] < best_score:\n",
    "            best_score=history.history['val_mse'][-1]\n",
    "            best_params=combinaison\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 6ms/step - loss: 13.0322 - mse: 13.0197 - val_loss: 12.0001 - val_mse: 11.9887\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 11.4487 - mse: 11.4208 - val_loss: 9.1667 - val_mse: 9.0335\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 8.1550 - mse: 7.9553 - val_loss: 5.5305 - val_mse: 5.1050\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 4.8745 - mse: 4.3729 - val_loss: 3.8806 - val_mse: 3.1754\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 3.5237 - mse: 2.7671 - val_loss: 3.3517 - val_mse: 2.4657\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 3.0607 - mse: 2.1423 - val_loss: 3.1158 - val_mse: 2.1131\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.8345 - mse: 1.8090 - val_loss: 2.9834 - val_mse: 1.8992\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.7110 - mse: 1.6105 - val_loss: 2.8998 - val_mse: 1.7564\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.6159 - mse: 1.4609 - val_loss: 2.8403 - val_mse: 1.6541\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.5688 - mse: 1.3737 - val_loss: 2.7966 - val_mse: 1.5812\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.5189 - mse: 1.2975 - val_loss: 2.7582 - val_mse: 1.5216\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.4767 - mse: 1.2368 - val_loss: 2.7259 - val_mse: 1.4757\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.4357 - mse: 1.1833 - val_loss: 2.6955 - val_mse: 1.4380\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.4082 - mse: 1.1489 - val_loss: 2.6670 - val_mse: 1.4064\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.3886 - mse: 1.1278 - val_loss: 2.6394 - val_mse: 1.3786\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.3650 - mse: 1.1044 - val_loss: 2.6130 - val_mse: 1.3550\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.3467 - mse: 1.0902 - val_loss: 2.5860 - val_mse: 1.3334\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.3116 - mse: 1.0609 - val_loss: 2.5593 - val_mse: 1.3137\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.2971 - mse: 1.0539 - val_loss: 2.5321 - val_mse: 1.2947\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.2745 - mse: 1.0398 - val_loss: 2.5066 - val_mse: 1.2786\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.2485 - mse: 1.0235 - val_loss: 2.4803 - val_mse: 1.2631\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.2307 - mse: 1.0160 - val_loss: 2.4533 - val_mse: 1.2482\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.2115 - mse: 1.0091 - val_loss: 2.4286 - val_mse: 1.2344\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.1943 - mse: 1.0033 - val_loss: 2.4036 - val_mse: 1.2215\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.1736 - mse: 0.9945 - val_loss: 2.3768 - val_mse: 1.2066\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.1484 - mse: 0.9816 - val_loss: 2.3529 - val_mse: 1.1950\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.1366 - mse: 0.9823 - val_loss: 2.3284 - val_mse: 1.1821\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.1104 - mse: 0.9673 - val_loss: 2.3036 - val_mse: 1.1709\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0869 - mse: 0.9571 - val_loss: 2.2814 - val_mse: 1.1605\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0682 - mse: 0.9505 - val_loss: 2.2593 - val_mse: 1.1495\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0552 - mse: 0.9485 - val_loss: 2.2356 - val_mse: 1.1379\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0388 - mse: 0.9442 - val_loss: 2.2159 - val_mse: 1.1283\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0175 - mse: 0.9332 - val_loss: 2.1951 - val_mse: 1.1189\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0011 - mse: 0.9275 - val_loss: 2.1759 - val_mse: 1.1109\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.9807 - mse: 0.9185 - val_loss: 2.1572 - val_mse: 1.1015\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.9675 - mse: 0.9140 - val_loss: 2.1393 - val_mse: 1.0937\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.9617 - mse: 0.9181 - val_loss: 2.1226 - val_mse: 1.0862\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.9385 - mse: 0.9043 - val_loss: 2.1051 - val_mse: 1.0777\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.9246 - mse: 0.8993 - val_loss: 2.0901 - val_mse: 1.0706\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.9185 - mse: 0.9010 - val_loss: 2.0748 - val_mse: 1.0636\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8987 - mse: 0.8891 - val_loss: 2.0613 - val_mse: 1.0575\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8941 - mse: 0.8922 - val_loss: 2.0468 - val_mse: 1.0505\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8801 - mse: 0.8852 - val_loss: 2.0346 - val_mse: 1.0447\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8610 - mse: 0.8730 - val_loss: 2.0225 - val_mse: 1.0391\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8575 - mse: 0.8756 - val_loss: 2.0107 - val_mse: 1.0334\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8470 - mse: 0.8707 - val_loss: 1.9993 - val_mse: 1.0272\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8358 - mse: 0.8651 - val_loss: 1.9892 - val_mse: 1.0226\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8249 - mse: 0.8591 - val_loss: 1.9790 - val_mse: 1.0171\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8223 - mse: 0.8620 - val_loss: 1.9692 - val_mse: 1.0124\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 1.8129 - mse: 0.8571 - val_loss: 1.9611 - val_mse: 1.0083\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8034 - mse: 0.8517 - val_loss: 1.9521 - val_mse: 1.0034\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7868 - mse: 0.8386 - val_loss: 1.9442 - val_mse: 0.9990\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7986 - mse: 0.8543 - val_loss: 1.9367 - val_mse: 0.9952\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7895 - mse: 0.8483 - val_loss: 1.9286 - val_mse: 0.9906\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7742 - mse: 0.8364 - val_loss: 1.9223 - val_mse: 0.9874\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7729 - mse: 0.8383 - val_loss: 1.9156 - val_mse: 0.9828\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7565 - mse: 0.8240 - val_loss: 1.9099 - val_mse: 0.9799\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7561 - mse: 0.8264 - val_loss: 1.9034 - val_mse: 0.9756\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7486 - mse: 0.8208 - val_loss: 1.8987 - val_mse: 0.9728\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7451 - mse: 0.8196 - val_loss: 1.8934 - val_mse: 0.9692\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7488 - mse: 0.8251 - val_loss: 1.8889 - val_mse: 0.9663\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7362 - mse: 0.8138 - val_loss: 1.8842 - val_mse: 0.9634\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7410 - mse: 0.8201 - val_loss: 1.8808 - val_mse: 0.9614\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7311 - mse: 0.8115 - val_loss: 1.8767 - val_mse: 0.9584\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7220 - mse: 0.8037 - val_loss: 1.8727 - val_mse: 0.9552\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7149 - mse: 0.7977 - val_loss: 1.8701 - val_mse: 0.9534\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7155 - mse: 0.7990 - val_loss: 1.8662 - val_mse: 0.9503\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7164 - mse: 0.8001 - val_loss: 1.8632 - val_mse: 0.9481\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7131 - mse: 0.7972 - val_loss: 1.8607 - val_mse: 0.9461\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7078 - mse: 0.7935 - val_loss: 1.8579 - val_mse: 0.9437\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7047 - mse: 0.7903 - val_loss: 1.8557 - val_mse: 0.9416\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6969 - mse: 0.7829 - val_loss: 1.8540 - val_mse: 0.9400\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7026 - mse: 0.7884 - val_loss: 1.8521 - val_mse: 0.9385\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7011 - mse: 0.7873 - val_loss: 1.8500 - val_mse: 0.9363\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6901 - mse: 0.7766 - val_loss: 1.8482 - val_mse: 0.9343\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6887 - mse: 0.7743 - val_loss: 1.8467 - val_mse: 0.9325\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6840 - mse: 0.7701 - val_loss: 1.8453 - val_mse: 0.9313\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6943 - mse: 0.7798 - val_loss: 1.8444 - val_mse: 0.9299\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6806 - mse: 0.7661 - val_loss: 1.8431 - val_mse: 0.9284\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6789 - mse: 0.7638 - val_loss: 1.8418 - val_mse: 0.9269\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6818 - mse: 0.7659 - val_loss: 1.8412 - val_mse: 0.9254\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6845 - mse: 0.7685 - val_loss: 1.8402 - val_mse: 0.9239\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6798 - mse: 0.7630 - val_loss: 1.8393 - val_mse: 0.9230\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6792 - mse: 0.7625 - val_loss: 1.8389 - val_mse: 0.9216\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6704 - mse: 0.7532 - val_loss: 1.8383 - val_mse: 0.9203\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6716 - mse: 0.7533 - val_loss: 1.8381 - val_mse: 0.9199\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6719 - mse: 0.7532 - val_loss: 1.8374 - val_mse: 0.9185\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6738 - mse: 0.7543 - val_loss: 1.8371 - val_mse: 0.9173\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6686 - mse: 0.7483 - val_loss: 1.8366 - val_mse: 0.9163\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6690 - mse: 0.7486 - val_loss: 1.8367 - val_mse: 0.9156\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6690 - mse: 0.7471 - val_loss: 1.8363 - val_mse: 0.9148\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6669 - mse: 0.7449 - val_loss: 1.8362 - val_mse: 0.9135\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6659 - mse: 0.7428 - val_loss: 1.8356 - val_mse: 0.9123\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6637 - mse: 0.7399 - val_loss: 1.8355 - val_mse: 0.9117\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6583 - mse: 0.7341 - val_loss: 1.8357 - val_mse: 0.9111\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6654 - mse: 0.7403 - val_loss: 1.8360 - val_mse: 0.9104\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6572 - mse: 0.7315 - val_loss: 1.8359 - val_mse: 0.9097\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6584 - mse: 0.7318 - val_loss: 1.8362 - val_mse: 0.9093\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6628 - mse: 0.7351 - val_loss: 1.8356 - val_mse: 0.9080\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6629 - mse: 0.7346 - val_loss: 1.8358 - val_mse: 0.9075\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6563 - mse: 0.7281 - val_loss: 1.8362 - val_mse: 0.9071\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6553 - mse: 0.7258 - val_loss: 1.8361 - val_mse: 0.9063\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6593 - mse: 0.7289 - val_loss: 1.8365 - val_mse: 0.9059\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6491 - mse: 0.7182 - val_loss: 1.8365 - val_mse: 0.9054\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6484 - mse: 0.7171 - val_loss: 1.8365 - val_mse: 0.9048\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6577 - mse: 0.7260 - val_loss: 1.8366 - val_mse: 0.9039\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6504 - mse: 0.7172 - val_loss: 1.8368 - val_mse: 0.9037\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6513 - mse: 0.7182 - val_loss: 1.8369 - val_mse: 0.9032\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6453 - mse: 0.7115 - val_loss: 1.8373 - val_mse: 0.9027\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6525 - mse: 0.7178 - val_loss: 1.8376 - val_mse: 0.9026\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6566 - mse: 0.7210 - val_loss: 1.8377 - val_mse: 0.9016\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6486 - mse: 0.7124 - val_loss: 1.8379 - val_mse: 0.9016\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6495 - mse: 0.7128 - val_loss: 1.8378 - val_mse: 0.9010\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6495 - mse: 0.7123 - val_loss: 1.8383 - val_mse: 0.9011\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6513 - mse: 0.7135 - val_loss: 1.8383 - val_mse: 0.9002\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6509 - mse: 0.7126 - val_loss: 1.8391 - val_mse: 0.9002\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6548 - mse: 0.7158 - val_loss: 1.8390 - val_mse: 0.8996\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6509 - mse: 0.7112 - val_loss: 1.8392 - val_mse: 0.8993\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6520 - mse: 0.7114 - val_loss: 1.8397 - val_mse: 0.8991\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6496 - mse: 0.7090 - val_loss: 1.8397 - val_mse: 0.8988\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6366 - mse: 0.6956 - val_loss: 1.8403 - val_mse: 0.8988\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6499 - mse: 0.7078 - val_loss: 1.8402 - val_mse: 0.8981\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6454 - mse: 0.7031 - val_loss: 1.8410 - val_mse: 0.8982\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6474 - mse: 0.7042 - val_loss: 1.8409 - val_mse: 0.8979\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6431 - mse: 0.6999 - val_loss: 1.8406 - val_mse: 0.8973\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6450 - mse: 0.7011 - val_loss: 1.8414 - val_mse: 0.8973\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6411 - mse: 0.6965 - val_loss: 1.8415 - val_mse: 0.8968\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6455 - mse: 0.7007 - val_loss: 1.8419 - val_mse: 0.8967\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6446 - mse: 0.6991 - val_loss: 1.8425 - val_mse: 0.8969\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6441 - mse: 0.6983 - val_loss: 1.8425 - val_mse: 0.8966\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6472 - mse: 0.7008 - val_loss: 1.8425 - val_mse: 0.8963\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6466 - mse: 0.6998 - val_loss: 1.8430 - val_mse: 0.8961\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6435 - mse: 0.6961 - val_loss: 1.8437 - val_mse: 0.8964\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6423 - mse: 0.6947 - val_loss: 1.8434 - val_mse: 0.8956\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6435 - mse: 0.6957 - val_loss: 1.8440 - val_mse: 0.8955\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6467 - mse: 0.6979 - val_loss: 1.8443 - val_mse: 0.8955\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6448 - mse: 0.6954 - val_loss: 1.8445 - val_mse: 0.8951\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6400 - mse: 0.6906 - val_loss: 1.8448 - val_mse: 0.8952\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6358 - mse: 0.6864 - val_loss: 1.8452 - val_mse: 0.8951\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6496 - mse: 0.6991 - val_loss: 1.8453 - val_mse: 0.8947\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6445 - mse: 0.6936 - val_loss: 1.8456 - val_mse: 0.8948\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6408 - mse: 0.6902 - val_loss: 1.8457 - val_mse: 0.8942\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6376 - mse: 0.6857 - val_loss: 1.8460 - val_mse: 0.8943\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6456 - mse: 0.6936 - val_loss: 1.8465 - val_mse: 0.8942\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6426 - mse: 0.6901 - val_loss: 1.8465 - val_mse: 0.8940\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6393 - mse: 0.6865 - val_loss: 1.8469 - val_mse: 0.8937\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6378 - mse: 0.6848 - val_loss: 1.8474 - val_mse: 0.8939\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6401 - mse: 0.6865 - val_loss: 1.8477 - val_mse: 0.8940\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6389 - mse: 0.6851 - val_loss: 1.8477 - val_mse: 0.8934\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6465 - mse: 0.6920 - val_loss: 1.8481 - val_mse: 0.8938\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6444 - mse: 0.6895 - val_loss: 1.8482 - val_mse: 0.8934\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6351 - mse: 0.6802 - val_loss: 1.8486 - val_mse: 0.8937\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6485 - mse: 0.6933 - val_loss: 1.8491 - val_mse: 0.8936\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6412 - mse: 0.6854 - val_loss: 1.8491 - val_mse: 0.8932\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6366 - mse: 0.6805 - val_loss: 1.8491 - val_mse: 0.8929\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6391 - mse: 0.6829 - val_loss: 1.8496 - val_mse: 0.8932\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6450 - mse: 0.6884 - val_loss: 1.8499 - val_mse: 0.8931\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6325 - mse: 0.6755 - val_loss: 1.8502 - val_mse: 0.8932\n",
      "Epoch 159/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6458 - mse: 0.6884 - val_loss: 1.8501 - val_mse: 0.8926\n",
      "Epoch 160/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6372 - mse: 0.6796 - val_loss: 1.8509 - val_mse: 0.8931\n",
      "Epoch 161/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6325 - mse: 0.6747 - val_loss: 1.8509 - val_mse: 0.8930\n",
      "Epoch 162/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6426 - mse: 0.6844 - val_loss: 1.8508 - val_mse: 0.8926\n",
      "Epoch 163/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6403 - mse: 0.6817 - val_loss: 1.8510 - val_mse: 0.8923\n",
      "Epoch 164/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6448 - mse: 0.6861 - val_loss: 1.8516 - val_mse: 0.8927\n",
      "Epoch 165/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6381 - mse: 0.6793 - val_loss: 1.8520 - val_mse: 0.8929\n",
      "Epoch 166/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6446 - mse: 0.6852 - val_loss: 1.8517 - val_mse: 0.8925\n",
      "Epoch 167/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6404 - mse: 0.6810 - val_loss: 1.8522 - val_mse: 0.8928\n",
      "Epoch 168/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6458 - mse: 0.6860 - val_loss: 1.8525 - val_mse: 0.8927\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 6ms/step - loss: 13.0700 - mse: 13.0663 - val_loss: 12.0336 - val_mse: 12.0297\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 11.5472 - mse: 11.5392 - val_loss: 9.1899 - val_mse: 9.1551\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 8.1178 - mse: 8.0655 - val_loss: 5.2264 - val_mse: 5.1129\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 4.4220 - mse: 4.2866 - val_loss: 3.1825 - val_mse: 2.9858\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.7382 - mse: 2.5248 - val_loss: 2.4767 - val_mse: 2.2198\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.1272 - mse: 1.8585 - val_loss: 2.1538 - val_mse: 1.8538\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8144 - mse: 1.5057 - val_loss: 1.9701 - val_mse: 1.6365\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.6338 - mse: 1.2930 - val_loss: 1.8546 - val_mse: 1.4936\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.5141 - mse: 1.1471 - val_loss: 1.7766 - val_mse: 1.3929\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.4439 - mse: 1.0553 - val_loss: 1.7219 - val_mse: 1.3192\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.3765 - mse: 0.9694 - val_loss: 1.6828 - val_mse: 1.2643\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.3328 - mse: 0.9109 - val_loss: 1.6539 - val_mse: 1.2218\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.3010 - mse: 0.8658 - val_loss: 1.6316 - val_mse: 1.1885\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.2644 - mse: 0.8188 - val_loss: 1.6141 - val_mse: 1.1614\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.2551 - mse: 0.8003 - val_loss: 1.6013 - val_mse: 1.1411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.2267 - mse: 0.7646 - val_loss: 1.5896 - val_mse: 1.1229\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.2170 - mse: 0.7489 - val_loss: 1.5796 - val_mse: 1.1079\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.2014 - mse: 0.7286 - val_loss: 1.5711 - val_mse: 1.0953\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1880 - mse: 0.7113 - val_loss: 1.5638 - val_mse: 1.0847\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1780 - mse: 0.6983 - val_loss: 1.5571 - val_mse: 1.0756\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1687 - mse: 0.6868 - val_loss: 1.5502 - val_mse: 1.0669\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1569 - mse: 0.6735 - val_loss: 1.5444 - val_mse: 1.0600\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1531 - mse: 0.6687 - val_loss: 1.5389 - val_mse: 1.0537\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1492 - mse: 0.6640 - val_loss: 1.5326 - val_mse: 1.0475\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1437 - mse: 0.6587 - val_loss: 1.5285 - val_mse: 1.0435\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1310 - mse: 0.6465 - val_loss: 1.5217 - val_mse: 1.0375\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1254 - mse: 0.6413 - val_loss: 1.5162 - val_mse: 1.0330\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1197 - mse: 0.6369 - val_loss: 1.5107 - val_mse: 1.0285\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1028 - mse: 0.6208 - val_loss: 1.5053 - val_mse: 1.0245\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1064 - mse: 0.6262 - val_loss: 1.5007 - val_mse: 1.0211\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0988 - mse: 0.6198 - val_loss: 1.4956 - val_mse: 1.0176\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0907 - mse: 0.6132 - val_loss: 1.4907 - val_mse: 1.0142\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0813 - mse: 0.6053 - val_loss: 1.4855 - val_mse: 1.0110\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0813 - mse: 0.6073 - val_loss: 1.4807 - val_mse: 1.0077\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0691 - mse: 0.5967 - val_loss: 1.4746 - val_mse: 1.0034\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0646 - mse: 0.5939 - val_loss: 1.4699 - val_mse: 1.0005\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0601 - mse: 0.5913 - val_loss: 1.4654 - val_mse: 0.9977\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0493 - mse: 0.5820 - val_loss: 1.4607 - val_mse: 0.9946\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0535 - mse: 0.5880 - val_loss: 1.4553 - val_mse: 0.9910\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0447 - mse: 0.5809 - val_loss: 1.4502 - val_mse: 0.9875\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0351 - mse: 0.5727 - val_loss: 1.4471 - val_mse: 0.9860\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0236 - mse: 0.5629 - val_loss: 1.4444 - val_mse: 0.9849\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0244 - mse: 0.5654 - val_loss: 1.4388 - val_mse: 0.9805\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0115 - mse: 0.5535 - val_loss: 1.4342 - val_mse: 0.9774\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0089 - mse: 0.5527 - val_loss: 1.4294 - val_mse: 0.9739\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0072 - mse: 0.5522 - val_loss: 1.4255 - val_mse: 0.9713\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9975 - mse: 0.5437 - val_loss: 1.4210 - val_mse: 0.9680\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9969 - mse: 0.5444 - val_loss: 1.4179 - val_mse: 0.9656\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9850 - mse: 0.5330 - val_loss: 1.4144 - val_mse: 0.9633\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9713 - mse: 0.5208 - val_loss: 1.4108 - val_mse: 0.9607\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9738 - mse: 0.5240 - val_loss: 1.4074 - val_mse: 0.9580\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9688 - mse: 0.5198 - val_loss: 1.4030 - val_mse: 0.9545\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9585 - mse: 0.5102 - val_loss: 1.4003 - val_mse: 0.9526\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9526 - mse: 0.5053 - val_loss: 1.3967 - val_mse: 0.9497\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9457 - mse: 0.4988 - val_loss: 1.3938 - val_mse: 0.9473\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9453 - mse: 0.4990 - val_loss: 1.3913 - val_mse: 0.9453\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9399 - mse: 0.4942 - val_loss: 1.3895 - val_mse: 0.9440\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9337 - mse: 0.4883 - val_loss: 1.3868 - val_mse: 0.9417\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9236 - mse: 0.4788 - val_loss: 1.3842 - val_mse: 0.9395\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9205 - mse: 0.4759 - val_loss: 1.3822 - val_mse: 0.9379\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9088 - mse: 0.4646 - val_loss: 1.3810 - val_mse: 0.9369\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9096 - mse: 0.4657 - val_loss: 1.3779 - val_mse: 0.9338\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9068 - mse: 0.4629 - val_loss: 1.3758 - val_mse: 0.9318\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8963 - mse: 0.4525 - val_loss: 1.3751 - val_mse: 0.9315\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8919 - mse: 0.4485 - val_loss: 1.3727 - val_mse: 0.9292\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8875 - mse: 0.4442 - val_loss: 1.3726 - val_mse: 0.9290\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8832 - mse: 0.4397 - val_loss: 1.3699 - val_mse: 0.9264\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8756 - mse: 0.4322 - val_loss: 1.3691 - val_mse: 0.9258\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8711 - mse: 0.4278 - val_loss: 1.3685 - val_mse: 0.9253\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8602 - mse: 0.4170 - val_loss: 1.3680 - val_mse: 0.9244\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8602 - mse: 0.4168 - val_loss: 1.3667 - val_mse: 0.9232\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8575 - mse: 0.4139 - val_loss: 1.3664 - val_mse: 0.9229\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8553 - mse: 0.4119 - val_loss: 1.3661 - val_mse: 0.9223\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8460 - mse: 0.4024 - val_loss: 1.3655 - val_mse: 0.9218\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8404 - mse: 0.3966 - val_loss: 1.3655 - val_mse: 0.9217\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8371 - mse: 0.3934 - val_loss: 1.3651 - val_mse: 0.9211\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8382 - mse: 0.3942 - val_loss: 1.3650 - val_mse: 0.9208\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8299 - mse: 0.3856 - val_loss: 1.3646 - val_mse: 0.9204\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8274 - mse: 0.3832 - val_loss: 1.3650 - val_mse: 0.9207\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8268 - mse: 0.3826 - val_loss: 1.3646 - val_mse: 0.9200\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8157 - mse: 0.3712 - val_loss: 1.3651 - val_mse: 0.9204\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8192 - mse: 0.3745 - val_loss: 1.3650 - val_mse: 0.9202\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8118 - mse: 0.3670 - val_loss: 1.3648 - val_mse: 0.9198\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8100 - mse: 0.3651 - val_loss: 1.3656 - val_mse: 0.9207\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8015 - mse: 0.3564 - val_loss: 1.3658 - val_mse: 0.9207\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8025 - mse: 0.3574 - val_loss: 1.3657 - val_mse: 0.9204\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8036 - mse: 0.3583 - val_loss: 1.3662 - val_mse: 0.9207\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7967 - mse: 0.3513 - val_loss: 1.3669 - val_mse: 0.9213\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 6ms/step - loss: 13.0570 - mse: 13.0553 - val_loss: 12.0707 - val_mse: 12.0690\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 11.6644 - mse: 11.6613 - val_loss: 9.3032 - val_mse: 9.2897\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 8.1230 - mse: 8.1019 - val_loss: 4.9935 - val_mse: 4.9453\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 4.1842 - mse: 4.1265 - val_loss: 2.9309 - val_mse: 2.8472\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.4992 - mse: 2.4086 - val_loss: 2.2378 - val_mse: 2.1290\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.8768 - mse: 1.7630 - val_loss: 1.9095 - val_mse: 1.7823\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.5840 - mse: 1.4529 - val_loss: 1.7154 - val_mse: 1.5733\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.3905 - mse: 1.2452 - val_loss: 1.5908 - val_mse: 1.4364\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.2590 - mse: 1.1019 - val_loss: 1.5073 - val_mse: 1.3421\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1704 - mse: 1.0028 - val_loss: 1.4475 - val_mse: 1.2732\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.1045 - mse: 0.9281 - val_loss: 1.4037 - val_mse: 1.2215\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0625 - mse: 0.8784 - val_loss: 1.3711 - val_mse: 1.1818\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0317 - mse: 0.8408 - val_loss: 1.3458 - val_mse: 1.1503\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9962 - mse: 0.7992 - val_loss: 1.3272 - val_mse: 1.1264\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9753 - mse: 0.7732 - val_loss: 1.3112 - val_mse: 1.1056\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9566 - mse: 0.7499 - val_loss: 1.2989 - val_mse: 1.0891\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9555 - mse: 0.7447 - val_loss: 1.2901 - val_mse: 1.0766\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9374 - mse: 0.7231 - val_loss: 1.2815 - val_mse: 1.0648\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9270 - mse: 0.7097 - val_loss: 1.2751 - val_mse: 1.0555\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9085 - mse: 0.6883 - val_loss: 1.2700 - val_mse: 1.0480\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9090 - mse: 0.6865 - val_loss: 1.2651 - val_mse: 1.0411\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9057 - mse: 0.6812 - val_loss: 1.2610 - val_mse: 1.0351\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9004 - mse: 0.6740 - val_loss: 1.2569 - val_mse: 1.0293\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8982 - mse: 0.6703 - val_loss: 1.2541 - val_mse: 1.0252\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8795 - mse: 0.6503 - val_loss: 1.2510 - val_mse: 1.0210\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8773 - mse: 0.6470 - val_loss: 1.2475 - val_mse: 1.0166\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8748 - mse: 0.6437 - val_loss: 1.2444 - val_mse: 1.0128\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8666 - mse: 0.6347 - val_loss: 1.2416 - val_mse: 1.0093\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8645 - mse: 0.6320 - val_loss: 1.2401 - val_mse: 1.0073\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8590 - mse: 0.6262 - val_loss: 1.2373 - val_mse: 1.0040\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8559 - mse: 0.6226 - val_loss: 1.2349 - val_mse: 1.0014\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8398 - mse: 0.6064 - val_loss: 1.2325 - val_mse: 0.9989\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8378 - mse: 0.6042 - val_loss: 1.2303 - val_mse: 0.9966\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8363 - mse: 0.6026 - val_loss: 1.2280 - val_mse: 0.9944\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8267 - mse: 0.5931 - val_loss: 1.2245 - val_mse: 0.9908\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8210 - mse: 0.5874 - val_loss: 1.2237 - val_mse: 0.9903\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8123 - mse: 0.5789 - val_loss: 1.2214 - val_mse: 0.9881\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8078 - mse: 0.5747 - val_loss: 1.2192 - val_mse: 0.9860\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7977 - mse: 0.5647 - val_loss: 1.2191 - val_mse: 0.9862\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7926 - mse: 0.5597 - val_loss: 1.2162 - val_mse: 0.9834\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7807 - mse: 0.5482 - val_loss: 1.2164 - val_mse: 0.9839\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7734 - mse: 0.5411 - val_loss: 1.2143 - val_mse: 0.9821\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7716 - mse: 0.5395 - val_loss: 1.2130 - val_mse: 0.9809\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7531 - mse: 0.5211 - val_loss: 1.2124 - val_mse: 0.9805\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7491 - mse: 0.5173 - val_loss: 1.2121 - val_mse: 0.9804\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7346 - mse: 0.5031 - val_loss: 1.2118 - val_mse: 0.9803\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7347 - mse: 0.5033 - val_loss: 1.2110 - val_mse: 0.9797\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7210 - mse: 0.4898 - val_loss: 1.2123 - val_mse: 0.9811\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7121 - mse: 0.4810 - val_loss: 1.2120 - val_mse: 0.9810\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7082 - mse: 0.4773 - val_loss: 1.2131 - val_mse: 0.9820\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6948 - mse: 0.4639 - val_loss: 1.2123 - val_mse: 0.9815\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6883 - mse: 0.4575 - val_loss: 1.2136 - val_mse: 0.9827\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 6ms/step - loss: 13.0281 - mse: 13.0043 - val_loss: 11.9553 - val_mse: 11.9378\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 11.2934 - mse: 11.2493 - val_loss: 8.0225 - val_mse: 7.7975\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 6.7329 - mse: 6.3983 - val_loss: 4.2714 - val_mse: 3.6061\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 3.8024 - mse: 3.0539 - val_loss: 3.4226 - val_mse: 2.4786\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 3.1023 - mse: 2.1134 - val_loss: 3.1530 - val_mse: 2.0545\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.8471 - mse: 1.7208 - val_loss: 3.0266 - val_mse: 1.8301\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.7179 - mse: 1.5034 - val_loss: 2.9551 - val_mse: 1.6958\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.6473 - mse: 1.3765 - val_loss: 2.9047 - val_mse: 1.6045\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.5879 - mse: 1.2809 - val_loss: 2.8666 - val_mse: 1.5407\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.5570 - mse: 1.2272 - val_loss: 2.8310 - val_mse: 1.4905\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.5204 - mse: 1.1776 - val_loss: 2.8012 - val_mse: 1.4539\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.4865 - mse: 1.1394 - val_loss: 2.7715 - val_mse: 1.4222\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.4513 - mse: 1.1033 - val_loss: 2.7423 - val_mse: 1.3961\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.4210 - mse: 1.0759 - val_loss: 2.7127 - val_mse: 1.3725\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.4040 - mse: 1.0654 - val_loss: 2.6843 - val_mse: 1.3525\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.3727 - mse: 1.0429 - val_loss: 2.6565 - val_mse: 1.3334\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.3419 - mse: 1.0220 - val_loss: 2.6272 - val_mse: 1.3158\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.3311 - mse: 1.0221 - val_loss: 2.5992 - val_mse: 1.3001\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.3082 - mse: 1.0119 - val_loss: 2.5723 - val_mse: 1.2859\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.2857 - mse: 1.0031 - val_loss: 2.5443 - val_mse: 1.2712\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.2479 - mse: 0.9776 - val_loss: 2.5175 - val_mse: 1.2587\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.2251 - mse: 0.9695 - val_loss: 2.4909 - val_mse: 1.2454\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.1968 - mse: 0.9544 - val_loss: 2.4643 - val_mse: 1.2320\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.1731 - mse: 0.9440 - val_loss: 2.4389 - val_mse: 1.2209\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.1528 - mse: 0.9381 - val_loss: 2.4135 - val_mse: 1.2091\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.1417 - mse: 0.9393 - val_loss: 2.3877 - val_mse: 1.1966\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.1058 - mse: 0.9180 - val_loss: 2.3639 - val_mse: 1.1854\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.0870 - mse: 0.9110 - val_loss: 2.3405 - val_mse: 1.1741\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.0707 - mse: 0.9080 - val_loss: 2.3169 - val_mse: 1.1619\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.0569 - mse: 0.9043 - val_loss: 2.2957 - val_mse: 1.1537\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.0383 - mse: 0.8990 - val_loss: 2.2729 - val_mse: 1.1421\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.0187 - mse: 0.8901 - val_loss: 2.2527 - val_mse: 1.1329\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9984 - mse: 0.8811 - val_loss: 2.2328 - val_mse: 1.1231\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9864 - mse: 0.8793 - val_loss: 2.2142 - val_mse: 1.1149\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9643 - mse: 0.8678 - val_loss: 2.1943 - val_mse: 1.1045\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9463 - mse: 0.8591 - val_loss: 2.1769 - val_mse: 1.0961\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9322 - mse: 0.8541 - val_loss: 2.1586 - val_mse: 1.0871\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9132 - mse: 0.8437 - val_loss: 2.1416 - val_mse: 1.0791\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.9025 - mse: 0.8424 - val_loss: 2.1250 - val_mse: 1.0704\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8916 - mse: 0.8378 - val_loss: 2.1101 - val_mse: 1.0636\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8837 - mse: 0.8390 - val_loss: 2.0946 - val_mse: 1.0565\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8716 - mse: 0.8347 - val_loss: 2.0813 - val_mse: 1.0500\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8491 - mse: 0.8195 - val_loss: 2.0673 - val_mse: 1.0414\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8521 - mse: 0.8280 - val_loss: 2.0550 - val_mse: 1.0355\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8373 - mse: 0.8191 - val_loss: 2.0421 - val_mse: 1.0290\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8196 - mse: 0.8080 - val_loss: 2.0302 - val_mse: 1.0229\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8107 - mse: 0.8043 - val_loss: 2.0186 - val_mse: 1.0166\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8060 - mse: 0.8049 - val_loss: 2.0075 - val_mse: 1.0103\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8034 - mse: 0.8063 - val_loss: 1.9985 - val_mse: 1.0056\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7826 - mse: 0.7909 - val_loss: 1.9900 - val_mse: 1.0016\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7800 - mse: 0.7924 - val_loss: 1.9803 - val_mse: 0.9955\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7742 - mse: 0.7905 - val_loss: 1.9720 - val_mse: 0.9902\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.7671 - mse: 0.7861 - val_loss: 1.9641 - val_mse: 0.9866\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7552 - mse: 0.7780 - val_loss: 1.9568 - val_mse: 0.9825\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7579 - mse: 0.7836 - val_loss: 1.9506 - val_mse: 0.9791\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7518 - mse: 0.7811 - val_loss: 1.9442 - val_mse: 0.9742\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7513 - mse: 0.7816 - val_loss: 1.9382 - val_mse: 0.9708\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7414 - mse: 0.7751 - val_loss: 1.9329 - val_mse: 0.9675\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7332 - mse: 0.7676 - val_loss: 1.9272 - val_mse: 0.9643\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7257 - mse: 0.7628 - val_loss: 1.9217 - val_mse: 0.9604\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7174 - mse: 0.7564 - val_loss: 1.9183 - val_mse: 0.9587\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7188 - mse: 0.7588 - val_loss: 1.9133 - val_mse: 0.9542\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7219 - mse: 0.7630 - val_loss: 1.9093 - val_mse: 0.9521\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7023 - mse: 0.7450 - val_loss: 1.9060 - val_mse: 0.9500\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6999 - mse: 0.7440 - val_loss: 1.9030 - val_mse: 0.9477\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7025 - mse: 0.7469 - val_loss: 1.8993 - val_mse: 0.9448\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7030 - mse: 0.7489 - val_loss: 1.8953 - val_mse: 0.9418\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6952 - mse: 0.7413 - val_loss: 1.8933 - val_mse: 0.9404\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6916 - mse: 0.7380 - val_loss: 1.8902 - val_mse: 0.9374\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6954 - mse: 0.7420 - val_loss: 1.8885 - val_mse: 0.9364\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6926 - mse: 0.7408 - val_loss: 1.8857 - val_mse: 0.9337\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6822 - mse: 0.7302 - val_loss: 1.8830 - val_mse: 0.9316\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6786 - mse: 0.7272 - val_loss: 1.8815 - val_mse: 0.9297\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6797 - mse: 0.7278 - val_loss: 1.8795 - val_mse: 0.9281\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6796 - mse: 0.7282 - val_loss: 1.8783 - val_mse: 0.9266\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6810 - mse: 0.7295 - val_loss: 1.8763 - val_mse: 0.9250\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6739 - mse: 0.7223 - val_loss: 1.8752 - val_mse: 0.9237\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6715 - mse: 0.7201 - val_loss: 1.8736 - val_mse: 0.9220\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6590 - mse: 0.7076 - val_loss: 1.8727 - val_mse: 0.9213\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6664 - mse: 0.7142 - val_loss: 1.8711 - val_mse: 0.9194\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6664 - mse: 0.7148 - val_loss: 1.8705 - val_mse: 0.9182\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6654 - mse: 0.7128 - val_loss: 1.8691 - val_mse: 0.9168\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6718 - mse: 0.7191 - val_loss: 1.8680 - val_mse: 0.9158\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6607 - mse: 0.7082 - val_loss: 1.8673 - val_mse: 0.9144\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6629 - mse: 0.7099 - val_loss: 1.8667 - val_mse: 0.9130\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6590 - mse: 0.7056 - val_loss: 1.8664 - val_mse: 0.9126\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6589 - mse: 0.7048 - val_loss: 1.8659 - val_mse: 0.9117\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6566 - mse: 0.7022 - val_loss: 1.8652 - val_mse: 0.9109\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6555 - mse: 0.7007 - val_loss: 1.8650 - val_mse: 0.9104\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6634 - mse: 0.7079 - val_loss: 1.8643 - val_mse: 0.9092\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6511 - mse: 0.6957 - val_loss: 1.8641 - val_mse: 0.9083\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6466 - mse: 0.6904 - val_loss: 1.8638 - val_mse: 0.9081\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6532 - mse: 0.6968 - val_loss: 1.8633 - val_mse: 0.9069\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6486 - mse: 0.6917 - val_loss: 1.8629 - val_mse: 0.9058\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6511 - mse: 0.6937 - val_loss: 1.8630 - val_mse: 0.9057\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6511 - mse: 0.6929 - val_loss: 1.8626 - val_mse: 0.9049\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6514 - mse: 0.6932 - val_loss: 1.8627 - val_mse: 0.9042\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6499 - mse: 0.6909 - val_loss: 1.8626 - val_mse: 0.9034\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6554 - mse: 0.6954 - val_loss: 1.8625 - val_mse: 0.9031\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6480 - mse: 0.6885 - val_loss: 1.8625 - val_mse: 0.9026\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6501 - mse: 0.6897 - val_loss: 1.8618 - val_mse: 0.9013\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6436 - mse: 0.6833 - val_loss: 1.8626 - val_mse: 0.9020\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6497 - mse: 0.6886 - val_loss: 1.8624 - val_mse: 0.9009\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6491 - mse: 0.6875 - val_loss: 1.8621 - val_mse: 0.9003\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6479 - mse: 0.6851 - val_loss: 1.8622 - val_mse: 0.9000\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6517 - mse: 0.6891 - val_loss: 1.8624 - val_mse: 0.8994\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6455 - mse: 0.6821 - val_loss: 1.8624 - val_mse: 0.8994\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6500 - mse: 0.6866 - val_loss: 1.8624 - val_mse: 0.8986\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6491 - mse: 0.6853 - val_loss: 1.8622 - val_mse: 0.8979\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6450 - mse: 0.6805 - val_loss: 1.8620 - val_mse: 0.8978\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6469 - mse: 0.6820 - val_loss: 1.8626 - val_mse: 0.8975\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6441 - mse: 0.6786 - val_loss: 1.8632 - val_mse: 0.8980\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6442 - mse: 0.6787 - val_loss: 1.8634 - val_mse: 0.8977\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6456 - mse: 0.6797 - val_loss: 1.8631 - val_mse: 0.8966\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6442 - mse: 0.6772 - val_loss: 1.8631 - val_mse: 0.8964\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6512 - mse: 0.6837 - val_loss: 1.8632 - val_mse: 0.8965\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6465 - mse: 0.6793 - val_loss: 1.8635 - val_mse: 0.8961\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6487 - mse: 0.6806 - val_loss: 1.8636 - val_mse: 0.8962\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6398 - mse: 0.6717 - val_loss: 1.8635 - val_mse: 0.8955\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6456 - mse: 0.6769 - val_loss: 1.8640 - val_mse: 0.8953\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6383 - mse: 0.6696 - val_loss: 1.8638 - val_mse: 0.8949\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6443 - mse: 0.6749 - val_loss: 1.8640 - val_mse: 0.8946\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6398 - mse: 0.6704 - val_loss: 1.8639 - val_mse: 0.8941\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6441 - mse: 0.6740 - val_loss: 1.8645 - val_mse: 0.8942\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6410 - mse: 0.6705 - val_loss: 1.8644 - val_mse: 0.8942\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6377 - mse: 0.6674 - val_loss: 1.8644 - val_mse: 0.8940\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6433 - mse: 0.6721 - val_loss: 1.8649 - val_mse: 0.8942\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6553 - mse: 0.6835 - val_loss: 1.8651 - val_mse: 0.8938\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6426 - mse: 0.6715 - val_loss: 1.8650 - val_mse: 0.8934\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6419 - mse: 0.6697 - val_loss: 1.8652 - val_mse: 0.8933\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6487 - mse: 0.6765 - val_loss: 1.8652 - val_mse: 0.8927\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6394 - mse: 0.6670 - val_loss: 1.8653 - val_mse: 0.8930\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6437 - mse: 0.6708 - val_loss: 1.8658 - val_mse: 0.8934\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6403 - mse: 0.6674 - val_loss: 1.8661 - val_mse: 0.8936\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6465 - mse: 0.6734 - val_loss: 1.8662 - val_mse: 0.8927\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6428 - mse: 0.6693 - val_loss: 1.8663 - val_mse: 0.8926\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6411 - mse: 0.6674 - val_loss: 1.8660 - val_mse: 0.8921\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6421 - mse: 0.6675 - val_loss: 1.8661 - val_mse: 0.8920\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6390 - mse: 0.6646 - val_loss: 1.8666 - val_mse: 0.8922\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6368 - mse: 0.6625 - val_loss: 1.8671 - val_mse: 0.8922\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6489 - mse: 0.6737 - val_loss: 1.8671 - val_mse: 0.8922\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6452 - mse: 0.6696 - val_loss: 1.8672 - val_mse: 0.8922\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6397 - mse: 0.6644 - val_loss: 1.8671 - val_mse: 0.8919\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6401 - mse: 0.6644 - val_loss: 1.8677 - val_mse: 0.8922\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6392 - mse: 0.6634 - val_loss: 1.8676 - val_mse: 0.8921\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6386 - mse: 0.6628 - val_loss: 1.8674 - val_mse: 0.8913\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6331 - mse: 0.6571 - val_loss: 1.8678 - val_mse: 0.8914\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6349 - mse: 0.6584 - val_loss: 1.8676 - val_mse: 0.8911\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6434 - mse: 0.6663 - val_loss: 1.8681 - val_mse: 0.8913\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6396 - mse: 0.6621 - val_loss: 1.8681 - val_mse: 0.8916\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6421 - mse: 0.6651 - val_loss: 1.8685 - val_mse: 0.8914\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6402 - mse: 0.6631 - val_loss: 1.8685 - val_mse: 0.8912\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.6374 - mse: 0.6596 - val_loss: 1.8688 - val_mse: 0.8913\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 6ms/step - loss: 13.0185 - mse: 13.0113 - val_loss: 11.9533 - val_mse: 11.9468\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 11.2925 - mse: 11.2795 - val_loss: 8.0374 - val_mse: 7.9790\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 6.6215 - mse: 6.5334 - val_loss: 3.6465 - val_mse: 3.4636\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 3.0447 - mse: 2.8357 - val_loss: 2.5033 - val_mse: 2.2287\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.1087 - mse: 1.8179 - val_loss: 2.1140 - val_mse: 1.7813\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.7635 - mse: 1.4189 - val_loss: 1.9243 - val_mse: 1.5486\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.5643 - mse: 1.1798 - val_loss: 1.8195 - val_mse: 1.4107\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.4409 - mse: 1.0255 - val_loss: 1.7542 - val_mse: 1.3195\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.3861 - mse: 0.9461 - val_loss: 1.7130 - val_mse: 1.2582\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.3364 - mse: 0.8775 - val_loss: 1.6854 - val_mse: 1.2145\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.2883 - mse: 0.8146 - val_loss: 1.6648 - val_mse: 1.1817\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.2674 - mse: 0.7818 - val_loss: 1.6498 - val_mse: 1.1570\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.2368 - mse: 0.7421 - val_loss: 1.6391 - val_mse: 1.1390\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.2239 - mse: 0.7220 - val_loss: 1.6314 - val_mse: 1.1258\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.2076 - mse: 0.7007 - val_loss: 1.6244 - val_mse: 1.1145\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1898 - mse: 0.6789 - val_loss: 1.6196 - val_mse: 1.1068\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1805 - mse: 0.6671 - val_loss: 1.6127 - val_mse: 1.0976\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1556 - mse: 0.6399 - val_loss: 1.6075 - val_mse: 1.0914\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1451 - mse: 0.6289 - val_loss: 1.6039 - val_mse: 1.0868\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1343 - mse: 0.6173 - val_loss: 1.5990 - val_mse: 1.0817\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1253 - mse: 0.6082 - val_loss: 1.5970 - val_mse: 1.0798\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1180 - mse: 0.6013 - val_loss: 1.5911 - val_mse: 1.0744\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1022 - mse: 0.5858 - val_loss: 1.5896 - val_mse: 1.0736\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0928 - mse: 0.5772 - val_loss: 1.5855 - val_mse: 1.0704\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0803 - mse: 0.5653 - val_loss: 1.5817 - val_mse: 1.0677\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0687 - mse: 0.5551 - val_loss: 1.5797 - val_mse: 1.0668\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0560 - mse: 0.5434 - val_loss: 1.5762 - val_mse: 1.0643\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0515 - mse: 0.5399 - val_loss: 1.5736 - val_mse: 1.0627\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0456 - mse: 0.5354 - val_loss: 1.5696 - val_mse: 1.0595\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0290 - mse: 0.5193 - val_loss: 1.5673 - val_mse: 1.0583\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0263 - mse: 0.5176 - val_loss: 1.5628 - val_mse: 1.0548\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0167 - mse: 0.5092 - val_loss: 1.5601 - val_mse: 1.0533\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0065 - mse: 0.5000 - val_loss: 1.5564 - val_mse: 1.0501\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9909 - mse: 0.4848 - val_loss: 1.5531 - val_mse: 1.0475\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9857 - mse: 0.4805 - val_loss: 1.5510 - val_mse: 1.0461\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9773 - mse: 0.4727 - val_loss: 1.5455 - val_mse: 1.0412\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9711 - mse: 0.4672 - val_loss: 1.5449 - val_mse: 1.0410\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9605 - mse: 0.4568 - val_loss: 1.5408 - val_mse: 1.0373\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9509 - mse: 0.4476 - val_loss: 1.5378 - val_mse: 1.0347\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9431 - mse: 0.4404 - val_loss: 1.5360 - val_mse: 1.0334\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9374 - mse: 0.4349 - val_loss: 1.5323 - val_mse: 1.0297\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9218 - mse: 0.4196 - val_loss: 1.5304 - val_mse: 1.0280\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9179 - mse: 0.4160 - val_loss: 1.5267 - val_mse: 1.0245\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9094 - mse: 0.4075 - val_loss: 1.5252 - val_mse: 1.0229\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8970 - mse: 0.3949 - val_loss: 1.5228 - val_mse: 1.0209\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8907 - mse: 0.3889 - val_loss: 1.5196 - val_mse: 1.0175\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8839 - mse: 0.3823 - val_loss: 1.5174 - val_mse: 1.0152\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8750 - mse: 0.3730 - val_loss: 1.5153 - val_mse: 1.0132\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8724 - mse: 0.3706 - val_loss: 1.5123 - val_mse: 1.0099\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.8642 - mse: 0.3623 - val_loss: 1.5112 - val_mse: 1.0085\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8549 - mse: 0.3527 - val_loss: 1.5075 - val_mse: 1.0049\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8500 - mse: 0.3478 - val_loss: 1.5069 - val_mse: 1.0041\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8447 - mse: 0.3422 - val_loss: 1.5047 - val_mse: 1.0018\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8343 - mse: 0.3319 - val_loss: 1.5022 - val_mse: 0.9995\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8288 - mse: 0.3262 - val_loss: 1.5011 - val_mse: 0.9979\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8216 - mse: 0.3189 - val_loss: 1.4986 - val_mse: 0.9954\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8195 - mse: 0.3165 - val_loss: 1.4979 - val_mse: 0.9944\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.8062 - mse: 0.3030 - val_loss: 1.4960 - val_mse: 0.9924\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8046 - mse: 0.3014 - val_loss: 1.4947 - val_mse: 0.9912\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7991 - mse: 0.2959 - val_loss: 1.4932 - val_mse: 0.9894\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7949 - mse: 0.2914 - val_loss: 1.4922 - val_mse: 0.9883\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7901 - mse: 0.2863 - val_loss: 1.4909 - val_mse: 0.9869\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7849 - mse: 0.2810 - val_loss: 1.4890 - val_mse: 0.9848\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7796 - mse: 0.2756 - val_loss: 1.4865 - val_mse: 0.9822\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7746 - mse: 0.2707 - val_loss: 1.4857 - val_mse: 0.9813\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7682 - mse: 0.2641 - val_loss: 1.4852 - val_mse: 0.9806\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7652 - mse: 0.2610 - val_loss: 1.4833 - val_mse: 0.9788\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7632 - mse: 0.2589 - val_loss: 1.4827 - val_mse: 0.9781\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7605 - mse: 0.2563 - val_loss: 1.4813 - val_mse: 0.9764\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7561 - mse: 0.2516 - val_loss: 1.4792 - val_mse: 0.9744\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7495 - mse: 0.2452 - val_loss: 1.4782 - val_mse: 0.9735\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7468 - mse: 0.2424 - val_loss: 1.4773 - val_mse: 0.9727\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7407 - mse: 0.2365 - val_loss: 1.4756 - val_mse: 0.9710\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7394 - mse: 0.2352 - val_loss: 1.4748 - val_mse: 0.9702\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7367 - mse: 0.2324 - val_loss: 1.4735 - val_mse: 0.9690\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7328 - mse: 0.2287 - val_loss: 1.4726 - val_mse: 0.9681\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7271 - mse: 0.2231 - val_loss: 1.4723 - val_mse: 0.9680\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7270 - mse: 0.2229 - val_loss: 1.4712 - val_mse: 0.9671\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7256 - mse: 0.2218 - val_loss: 1.4702 - val_mse: 0.9660\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7216 - mse: 0.2178 - val_loss: 1.4684 - val_mse: 0.9642\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7174 - mse: 0.2136 - val_loss: 1.4684 - val_mse: 0.9647\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7174 - mse: 0.2142 - val_loss: 1.4673 - val_mse: 0.9635\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7136 - mse: 0.2101 - val_loss: 1.4658 - val_mse: 0.9624\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7091 - mse: 0.2063 - val_loss: 1.4651 - val_mse: 0.9617\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7089 - mse: 0.2061 - val_loss: 1.4642 - val_mse: 0.9610\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7052 - mse: 0.2025 - val_loss: 1.4621 - val_mse: 0.9590\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.7042 - mse: 0.2016 - val_loss: 1.4614 - val_mse: 0.9586\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6995 - mse: 0.1971 - val_loss: 1.4606 - val_mse: 0.9583\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6982 - mse: 0.1963 - val_loss: 1.4601 - val_mse: 0.9578\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6960 - mse: 0.1943 - val_loss: 1.4588 - val_mse: 0.9568\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6955 - mse: 0.1937 - val_loss: 1.4581 - val_mse: 0.9565\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6950 - mse: 0.1938 - val_loss: 1.4571 - val_mse: 0.9556\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6892 - mse: 0.1882 - val_loss: 1.4555 - val_mse: 0.9544\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6894 - mse: 0.1886 - val_loss: 1.4550 - val_mse: 0.9541\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6878 - mse: 0.1874 - val_loss: 1.4542 - val_mse: 0.9536\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6860 - mse: 0.1858 - val_loss: 1.4533 - val_mse: 0.9529\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6817 - mse: 0.1818 - val_loss: 1.4528 - val_mse: 0.9528\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6800 - mse: 0.1804 - val_loss: 1.4510 - val_mse: 0.9512\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6809 - mse: 0.1814 - val_loss: 1.4508 - val_mse: 0.9513\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6786 - mse: 0.1797 - val_loss: 1.4496 - val_mse: 0.9505\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6782 - mse: 0.1793 - val_loss: 1.4491 - val_mse: 0.9504\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6750 - mse: 0.1767 - val_loss: 1.4471 - val_mse: 0.9486\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6763 - mse: 0.1783 - val_loss: 1.4474 - val_mse: 0.9490\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6754 - mse: 0.1773 - val_loss: 1.4460 - val_mse: 0.9481\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6726 - mse: 0.1751 - val_loss: 1.4460 - val_mse: 0.9484\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6695 - mse: 0.1724 - val_loss: 1.4457 - val_mse: 0.9483\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6701 - mse: 0.1733 - val_loss: 1.4446 - val_mse: 0.9477\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6692 - mse: 0.1725 - val_loss: 1.4439 - val_mse: 0.9471\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6684 - mse: 0.1720 - val_loss: 1.4425 - val_mse: 0.9460\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6650 - mse: 0.1689 - val_loss: 1.4423 - val_mse: 0.9462\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6653 - mse: 0.1696 - val_loss: 1.4418 - val_mse: 0.9459\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6646 - mse: 0.1690 - val_loss: 1.4406 - val_mse: 0.9449\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6633 - mse: 0.1682 - val_loss: 1.4397 - val_mse: 0.9444\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6608 - mse: 0.1659 - val_loss: 1.4393 - val_mse: 0.9443\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6601 - mse: 0.1653 - val_loss: 1.4388 - val_mse: 0.9440\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6573 - mse: 0.1629 - val_loss: 1.4380 - val_mse: 0.9435\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6591 - mse: 0.1650 - val_loss: 1.4377 - val_mse: 0.9434\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6571 - mse: 0.1633 - val_loss: 1.4365 - val_mse: 0.9424\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6554 - mse: 0.1618 - val_loss: 1.4360 - val_mse: 0.9422\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6540 - mse: 0.1607 - val_loss: 1.4355 - val_mse: 0.9420\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6565 - mse: 0.1632 - val_loss: 1.4341 - val_mse: 0.9408\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6534 - mse: 0.1605 - val_loss: 1.4337 - val_mse: 0.9405\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6529 - mse: 0.1602 - val_loss: 1.4341 - val_mse: 0.9413\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6525 - mse: 0.1601 - val_loss: 1.4329 - val_mse: 0.9403\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6514 - mse: 0.1593 - val_loss: 1.4321 - val_mse: 0.9397\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6507 - mse: 0.1588 - val_loss: 1.4310 - val_mse: 0.9390\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6512 - mse: 0.1597 - val_loss: 1.4309 - val_mse: 0.9390\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6508 - mse: 0.1591 - val_loss: 1.4301 - val_mse: 0.9385\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6482 - mse: 0.1571 - val_loss: 1.4295 - val_mse: 0.9383\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6482 - mse: 0.1574 - val_loss: 1.4296 - val_mse: 0.9384\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6474 - mse: 0.1565 - val_loss: 1.4289 - val_mse: 0.9378\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6463 - mse: 0.1557 - val_loss: 1.4280 - val_mse: 0.9372\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6457 - mse: 0.1551 - val_loss: 1.4272 - val_mse: 0.9367\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6450 - mse: 0.1548 - val_loss: 1.4264 - val_mse: 0.9360\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6449 - mse: 0.1548 - val_loss: 1.4259 - val_mse: 0.9357\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6460 - mse: 0.1560 - val_loss: 1.4263 - val_mse: 0.9365\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6449 - mse: 0.1554 - val_loss: 1.4254 - val_mse: 0.9356\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6425 - mse: 0.1531 - val_loss: 1.4250 - val_mse: 0.9355\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6419 - mse: 0.1527 - val_loss: 1.4245 - val_mse: 0.9351\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.6432 - mse: 0.1541 - val_loss: 1.4245 - val_mse: 0.9351\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6435 - mse: 0.1546 - val_loss: 1.4231 - val_mse: 0.9340\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6411 - mse: 0.1524 - val_loss: 1.4230 - val_mse: 0.9340\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6402 - mse: 0.1516 - val_loss: 1.4230 - val_mse: 0.9343\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6392 - mse: 0.1509 - val_loss: 1.4223 - val_mse: 0.9337\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6410 - mse: 0.1527 - val_loss: 1.4217 - val_mse: 0.9331\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6394 - mse: 0.1511 - val_loss: 1.4208 - val_mse: 0.9325\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6401 - mse: 0.1520 - val_loss: 1.4217 - val_mse: 0.9336\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6399 - mse: 0.1521 - val_loss: 1.4204 - val_mse: 0.9324\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6374 - mse: 0.1498 - val_loss: 1.4197 - val_mse: 0.9319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6380 - mse: 0.1504 - val_loss: 1.4192 - val_mse: 0.9314\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6374 - mse: 0.1499 - val_loss: 1.4186 - val_mse: 0.9309\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6368 - mse: 0.1496 - val_loss: 1.4188 - val_mse: 0.9313\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6349 - mse: 0.1478 - val_loss: 1.4185 - val_mse: 0.9311\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6356 - mse: 0.1486 - val_loss: 1.4185 - val_mse: 0.9312\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6348 - mse: 0.1479 - val_loss: 1.4174 - val_mse: 0.9303\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6346 - mse: 0.1480 - val_loss: 1.4168 - val_mse: 0.9298\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6349 - mse: 0.1481 - val_loss: 1.4171 - val_mse: 0.9302\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6329 - mse: 0.1464 - val_loss: 1.4161 - val_mse: 0.9293\n",
      "Epoch 159/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6344 - mse: 0.1479 - val_loss: 1.4161 - val_mse: 0.9294\n",
      "Epoch 160/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6352 - mse: 0.1487 - val_loss: 1.4152 - val_mse: 0.9285\n",
      "Epoch 161/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6320 - mse: 0.1456 - val_loss: 1.4151 - val_mse: 0.9285\n",
      "Epoch 162/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6327 - mse: 0.1465 - val_loss: 1.4149 - val_mse: 0.9285\n",
      "Epoch 163/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6340 - mse: 0.1480 - val_loss: 1.4145 - val_mse: 0.9281\n",
      "Epoch 164/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6337 - mse: 0.1476 - val_loss: 1.4137 - val_mse: 0.9275\n",
      "Epoch 165/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6307 - mse: 0.1448 - val_loss: 1.4138 - val_mse: 0.9278\n",
      "Epoch 166/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6325 - mse: 0.1467 - val_loss: 1.4134 - val_mse: 0.9274\n",
      "Epoch 167/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6318 - mse: 0.1461 - val_loss: 1.4133 - val_mse: 0.9274\n",
      "Epoch 168/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6316 - mse: 0.1459 - val_loss: 1.4134 - val_mse: 0.9276\n",
      "Epoch 169/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6311 - mse: 0.1455 - val_loss: 1.4127 - val_mse: 0.9268\n",
      "Epoch 170/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6330 - mse: 0.1475 - val_loss: 1.4125 - val_mse: 0.9265\n",
      "Epoch 171/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6315 - mse: 0.1460 - val_loss: 1.4126 - val_mse: 0.9270\n",
      "Epoch 172/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6304 - mse: 0.1450 - val_loss: 1.4120 - val_mse: 0.9264\n",
      "Epoch 173/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6309 - mse: 0.1455 - val_loss: 1.4120 - val_mse: 0.9266\n",
      "Epoch 174/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6308 - mse: 0.1455 - val_loss: 1.4105 - val_mse: 0.9251\n",
      "Epoch 175/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6290 - mse: 0.1439 - val_loss: 1.4113 - val_mse: 0.9258\n",
      "Epoch 176/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6303 - mse: 0.1450 - val_loss: 1.4104 - val_mse: 0.9253\n",
      "Epoch 177/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6290 - mse: 0.1441 - val_loss: 1.4105 - val_mse: 0.9254\n",
      "Epoch 178/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6290 - mse: 0.1441 - val_loss: 1.4110 - val_mse: 0.9258\n",
      "Epoch 179/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6286 - mse: 0.1437 - val_loss: 1.4104 - val_mse: 0.9253\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 2s 7ms/step - loss: 13.0765 - mse: 13.0732 - val_loss: 12.0195 - val_mse: 12.0164\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 11.3460 - mse: 11.3402 - val_loss: 7.9413 - val_mse: 7.9161\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 6.4337 - mse: 6.3961 - val_loss: 3.4504 - val_mse: 3.3731\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 2.8321 - mse: 2.7439 - val_loss: 2.2820 - val_mse: 2.1662\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.8750 - mse: 1.7523 - val_loss: 1.8673 - val_mse: 1.7260\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.4791 - mse: 1.3326 - val_loss: 1.6607 - val_mse: 1.4998\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.2767 - mse: 1.1116 - val_loss: 1.5399 - val_mse: 1.3632\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.1532 - mse: 0.9732 - val_loss: 1.4678 - val_mse: 1.2784\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0612 - mse: 0.8690 - val_loss: 1.4213 - val_mse: 1.2213\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0179 - mse: 0.8155 - val_loss: 1.3889 - val_mse: 1.1799\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9705 - mse: 0.7595 - val_loss: 1.3678 - val_mse: 1.1513\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9370 - mse: 0.7189 - val_loss: 1.3529 - val_mse: 1.1303\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9132 - mse: 0.6892 - val_loss: 1.3424 - val_mse: 1.1146\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8812 - mse: 0.6522 - val_loss: 1.3350 - val_mse: 1.1029\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8679 - mse: 0.6348 - val_loss: 1.3313 - val_mse: 1.0956\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8569 - mse: 0.6204 - val_loss: 1.3271 - val_mse: 1.0885\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8395 - mse: 0.6003 - val_loss: 1.3249 - val_mse: 1.0840\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8272 - mse: 0.5857 - val_loss: 1.3230 - val_mse: 1.0799\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8173 - mse: 0.5738 - val_loss: 1.3212 - val_mse: 1.0766\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.8043 - mse: 0.5594 - val_loss: 1.3216 - val_mse: 1.0755\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7965 - mse: 0.5503 - val_loss: 1.3201 - val_mse: 1.0731\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7845 - mse: 0.5373 - val_loss: 1.3183 - val_mse: 1.0706\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7789 - mse: 0.5309 - val_loss: 1.3185 - val_mse: 1.0700\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7659 - mse: 0.5173 - val_loss: 1.3177 - val_mse: 1.0687\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7529 - mse: 0.5038 - val_loss: 1.3179 - val_mse: 1.0684\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7443 - mse: 0.4948 - val_loss: 1.3181 - val_mse: 1.0682\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7346 - mse: 0.4847 - val_loss: 1.3167 - val_mse: 1.0664\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7297 - mse: 0.4794 - val_loss: 1.3177 - val_mse: 1.0672\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7216 - mse: 0.4712 - val_loss: 1.3175 - val_mse: 1.0667\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7116 - mse: 0.4609 - val_loss: 1.3184 - val_mse: 1.0674\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.7033 - mse: 0.4524 - val_loss: 1.3184 - val_mse: 1.0671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.6980 - mse: 0.4467 - val_loss: 1.3190 - val_mse: 1.0676\n"
     ]
    }
   ],
   "source": [
    "# Attention - le Grid Search est très long\n",
    "lambdas_ = [0.0002, 0.00005, 0.00002]\n",
    "ks = [15,30]\n",
    "\n",
    "param_grid = dict(k=ks, lambda_=lambdas_)\n",
    "\n",
    "best_params, best_score = grid_search(train, param_grid, get_mf_bias_l2_reg_model,\n",
    "                                      nb_users, nb_movies, validation_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters : (30, 0.0002)\n",
      "Best validation RMSE : 0.8913027048110962\n"
     ]
    }
   ],
   "source": [
    "print('Best hyper-parameters : ' + str(best_params))\n",
    "print('Best validation RMSE : ' + str(best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain with early stopping, higher number of epochs and batch size and corresponding best hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually other hyper-parameters such as the ones of SGD should also be grid-searched, like the number of epochs or the batch size. But that would be a bit long for this course. Retrain below your model on the whole dataset with the best values obtained from your grid search, including the test set, to make new predictions with our optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "178/178 [==============================] - 2s 6ms/step - loss: 13.0147 - mse: 12.9916 - val_loss: 12.0030 - val_mse: 11.9801\n",
      "Epoch 2/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 10.8556 - mse: 10.7914 - val_loss: 7.1099 - val_mse: 6.7954\n",
      "Epoch 3/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 5.7338 - mse: 5.2942 - val_loss: 3.9612 - val_mse: 3.1889\n",
      "Epoch 4/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 3.4820 - mse: 2.6363 - val_loss: 3.3273 - val_mse: 2.3098\n",
      "Epoch 5/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 2.9742 - mse: 1.9178 - val_loss: 3.0956 - val_mse: 1.9396\n",
      "Epoch 6/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 2.7687 - mse: 1.5896 - val_loss: 2.9814 - val_mse: 1.7391\n",
      "Epoch 7/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.6697 - mse: 1.4120 - val_loss: 2.9132 - val_mse: 1.6192\n",
      "Epoch 8/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.6008 - mse: 1.2977 - val_loss: 2.8631 - val_mse: 1.5371\n",
      "Epoch 9/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.5483 - mse: 1.2165 - val_loss: 2.8234 - val_mse: 1.4798\n",
      "Epoch 10/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.5077 - mse: 1.1623 - val_loss: 2.7872 - val_mse: 1.4354\n",
      "Epoch 11/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.4679 - mse: 1.1159 - val_loss: 2.7562 - val_mse: 1.4042\n",
      "Epoch 12/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.4371 - mse: 1.0859 - val_loss: 2.7233 - val_mse: 1.3755\n",
      "Epoch 13/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.3989 - mse: 1.0529 - val_loss: 2.6914 - val_mse: 1.3528\n",
      "Epoch 14/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.3752 - mse: 1.0388 - val_loss: 2.6614 - val_mse: 1.3324\n",
      "Epoch 15/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.3520 - mse: 1.0261 - val_loss: 2.6306 - val_mse: 1.3130\n",
      "Epoch 16/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.3194 - mse: 1.0049 - val_loss: 2.5999 - val_mse: 1.2962\n",
      "Epoch 17/500\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 2.2859 - mse: 0.9868 - val_loss: 2.5705 - val_mse: 1.2807\n",
      "Epoch 18/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.2729 - mse: 0.9874 - val_loss: 2.5398 - val_mse: 1.2652\n",
      "Epoch 19/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.2501 - mse: 0.9795 - val_loss: 2.5114 - val_mse: 1.2517\n",
      "Epoch 20/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.2268 - mse: 0.9715 - val_loss: 2.4839 - val_mse: 1.2397\n",
      "Epoch 21/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.1943 - mse: 0.9537 - val_loss: 2.4536 - val_mse: 1.2262\n",
      "Epoch 22/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.1746 - mse: 0.9515 - val_loss: 2.4264 - val_mse: 1.2132\n",
      "Epoch 23/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.1474 - mse: 0.9384 - val_loss: 2.3974 - val_mse: 1.1998\n",
      "Epoch 24/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.1261 - mse: 0.9322 - val_loss: 2.3716 - val_mse: 1.1899\n",
      "Epoch 25/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.1046 - mse: 0.9258 - val_loss: 2.3447 - val_mse: 1.1766\n",
      "Epoch 26/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 2.0841 - mse: 0.9191 - val_loss: 2.3180 - val_mse: 1.1644\n",
      "Epoch 27/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.0620 - mse: 0.9116 - val_loss: 2.2944 - val_mse: 1.1537\n",
      "Epoch 28/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.0342 - mse: 0.8973 - val_loss: 2.2702 - val_mse: 1.1438\n",
      "Epoch 29/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.0273 - mse: 0.9041 - val_loss: 2.2464 - val_mse: 1.1331\n",
      "Epoch 30/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 2.0125 - mse: 0.9019 - val_loss: 2.2237 - val_mse: 1.1225\n",
      "Epoch 31/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.9931 - mse: 0.8950 - val_loss: 2.2018 - val_mse: 1.1120\n",
      "Epoch 32/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.9742 - mse: 0.8877 - val_loss: 2.1810 - val_mse: 1.1034\n",
      "Epoch 33/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.9532 - mse: 0.8780 - val_loss: 2.1619 - val_mse: 1.0951\n",
      "Epoch 34/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.9400 - mse: 0.8761 - val_loss: 2.1432 - val_mse: 1.0868\n",
      "Epoch 35/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.9248 - mse: 0.8710 - val_loss: 2.1250 - val_mse: 1.0787\n",
      "Epoch 36/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.9130 - mse: 0.8687 - val_loss: 2.1070 - val_mse: 1.0704\n",
      "Epoch 37/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8854 - mse: 0.8511 - val_loss: 2.0899 - val_mse: 1.0605\n",
      "Epoch 38/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.8768 - mse: 0.8496 - val_loss: 2.0738 - val_mse: 1.0538\n",
      "Epoch 39/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8598 - mse: 0.8409 - val_loss: 2.0594 - val_mse: 1.0472\n",
      "Epoch 40/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8562 - mse: 0.8456 - val_loss: 2.0443 - val_mse: 1.0395\n",
      "Epoch 41/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8521 - mse: 0.8497 - val_loss: 2.0306 - val_mse: 1.0315\n",
      "Epoch 42/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8323 - mse: 0.8352 - val_loss: 2.0185 - val_mse: 1.0261\n",
      "Epoch 43/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8301 - mse: 0.8393 - val_loss: 2.0059 - val_mse: 1.0198\n",
      "Epoch 44/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8108 - mse: 0.8258 - val_loss: 1.9961 - val_mse: 1.0162\n",
      "Epoch 45/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8072 - mse: 0.8277 - val_loss: 1.9858 - val_mse: 1.0105\n",
      "Epoch 46/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7955 - mse: 0.8210 - val_loss: 1.9751 - val_mse: 1.0041\n",
      "Epoch 47/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7810 - mse: 0.8109 - val_loss: 1.9672 - val_mse: 1.0007\n",
      "Epoch 48/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7714 - mse: 0.8055 - val_loss: 1.9587 - val_mse: 0.9961\n",
      "Epoch 49/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7667 - mse: 0.8047 - val_loss: 1.9502 - val_mse: 0.9903\n",
      "Epoch 50/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7603 - mse: 0.8008 - val_loss: 1.9423 - val_mse: 0.9861\n",
      "Epoch 51/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7578 - mse: 0.8019 - val_loss: 1.9358 - val_mse: 0.9829\n",
      "Epoch 52/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7565 - mse: 0.8035 - val_loss: 1.9293 - val_mse: 0.9786\n",
      "Epoch 53/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7439 - mse: 0.7935 - val_loss: 1.9224 - val_mse: 0.9738\n",
      "Epoch 54/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.7338 - mse: 0.7858 - val_loss: 1.9176 - val_mse: 0.9714\n",
      "Epoch 55/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7360 - mse: 0.7899 - val_loss: 1.9115 - val_mse: 0.9668\n",
      "Epoch 56/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7267 - mse: 0.7823 - val_loss: 1.9070 - val_mse: 0.9640\n",
      "Epoch 57/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7265 - mse: 0.7828 - val_loss: 1.9017 - val_mse: 0.9605\n",
      "Epoch 58/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7193 - mse: 0.7783 - val_loss: 1.8983 - val_mse: 0.9585\n",
      "Epoch 59/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7064 - mse: 0.7667 - val_loss: 1.8941 - val_mse: 0.9547\n",
      "Epoch 60/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7065 - mse: 0.7669 - val_loss: 1.8906 - val_mse: 0.9527\n",
      "Epoch 61/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7089 - mse: 0.7706 - val_loss: 1.8869 - val_mse: 0.9491\n",
      "Epoch 62/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7056 - mse: 0.7675 - val_loss: 1.8844 - val_mse: 0.9476\n",
      "Epoch 63/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.7032 - mse: 0.7667 - val_loss: 1.8816 - val_mse: 0.9454\n",
      "Epoch 64/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6984 - mse: 0.7620 - val_loss: 1.8791 - val_mse: 0.9435\n",
      "Epoch 65/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6970 - mse: 0.7615 - val_loss: 1.8767 - val_mse: 0.9407\n",
      "Epoch 66/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6909 - mse: 0.7552 - val_loss: 1.8736 - val_mse: 0.9382\n",
      "Epoch 67/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6810 - mse: 0.7456 - val_loss: 1.8719 - val_mse: 0.9373\n",
      "Epoch 68/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6905 - mse: 0.7552 - val_loss: 1.8695 - val_mse: 0.9346\n",
      "Epoch 69/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6891 - mse: 0.7537 - val_loss: 1.8679 - val_mse: 0.9331\n",
      "Epoch 70/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6834 - mse: 0.7484 - val_loss: 1.8662 - val_mse: 0.9314\n",
      "Epoch 71/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6882 - mse: 0.7526 - val_loss: 1.8645 - val_mse: 0.9302\n",
      "Epoch 72/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6773 - mse: 0.7427 - val_loss: 1.8627 - val_mse: 0.9274\n",
      "Epoch 73/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6787 - mse: 0.7426 - val_loss: 1.8627 - val_mse: 0.9281\n",
      "Epoch 74/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6802 - mse: 0.7450 - val_loss: 1.8610 - val_mse: 0.9258\n",
      "Epoch 75/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6745 - mse: 0.7384 - val_loss: 1.8598 - val_mse: 0.9247\n",
      "Epoch 76/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6690 - mse: 0.7337 - val_loss: 1.8587 - val_mse: 0.9229\n",
      "Epoch 77/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6732 - mse: 0.7365 - val_loss: 1.8580 - val_mse: 0.9220\n",
      "Epoch 78/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6694 - mse: 0.7326 - val_loss: 1.8579 - val_mse: 0.9208\n",
      "Epoch 79/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6682 - mse: 0.7309 - val_loss: 1.8566 - val_mse: 0.9197\n",
      "Epoch 80/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6671 - mse: 0.7297 - val_loss: 1.8558 - val_mse: 0.9183\n",
      "Epoch 81/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6657 - mse: 0.7280 - val_loss: 1.8551 - val_mse: 0.9167\n",
      "Epoch 82/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6673 - mse: 0.7287 - val_loss: 1.8552 - val_mse: 0.9166\n",
      "Epoch 83/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6656 - mse: 0.7268 - val_loss: 1.8543 - val_mse: 0.9152\n",
      "Epoch 84/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6601 - mse: 0.7202 - val_loss: 1.8539 - val_mse: 0.9143\n",
      "Epoch 85/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6609 - mse: 0.7214 - val_loss: 1.8540 - val_mse: 0.9139\n",
      "Epoch 86/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6614 - mse: 0.7205 - val_loss: 1.8538 - val_mse: 0.9131\n",
      "Epoch 87/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6595 - mse: 0.7184 - val_loss: 1.8533 - val_mse: 0.9120\n",
      "Epoch 88/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6568 - mse: 0.7151 - val_loss: 1.8532 - val_mse: 0.9115\n",
      "Epoch 89/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6613 - mse: 0.7194 - val_loss: 1.8533 - val_mse: 0.9110\n",
      "Epoch 90/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6606 - mse: 0.7178 - val_loss: 1.8526 - val_mse: 0.9097\n",
      "Epoch 91/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6535 - mse: 0.7106 - val_loss: 1.8525 - val_mse: 0.9096\n",
      "Epoch 92/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6641 - mse: 0.7200 - val_loss: 1.8522 - val_mse: 0.9089\n",
      "Epoch 93/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6591 - mse: 0.7153 - val_loss: 1.8528 - val_mse: 0.9087\n",
      "Epoch 94/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6572 - mse: 0.7127 - val_loss: 1.8526 - val_mse: 0.9078\n",
      "Epoch 95/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6569 - mse: 0.7120 - val_loss: 1.8524 - val_mse: 0.9070\n",
      "Epoch 96/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6579 - mse: 0.7122 - val_loss: 1.8524 - val_mse: 0.9063\n",
      "Epoch 97/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6491 - mse: 0.7029 - val_loss: 1.8522 - val_mse: 0.9058\n",
      "Epoch 98/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6592 - mse: 0.7121 - val_loss: 1.8524 - val_mse: 0.9055\n",
      "Epoch 99/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6546 - mse: 0.7071 - val_loss: 1.8524 - val_mse: 0.9054\n",
      "Epoch 100/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6637 - mse: 0.7158 - val_loss: 1.8527 - val_mse: 0.9050\n",
      "Epoch 101/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6573 - mse: 0.7096 - val_loss: 1.8530 - val_mse: 0.9050\n",
      "Epoch 102/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6540 - mse: 0.7055 - val_loss: 1.8527 - val_mse: 0.9041\n",
      "Epoch 103/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6520 - mse: 0.7030 - val_loss: 1.8529 - val_mse: 0.9041\n",
      "Epoch 104/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6601 - mse: 0.7105 - val_loss: 1.8533 - val_mse: 0.9039\n",
      "Epoch 105/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6554 - mse: 0.7051 - val_loss: 1.8531 - val_mse: 0.9036\n",
      "Epoch 106/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6574 - mse: 0.7073 - val_loss: 1.8529 - val_mse: 0.9021\n",
      "Epoch 107/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6482 - mse: 0.6972 - val_loss: 1.8528 - val_mse: 0.9020\n",
      "Epoch 108/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6518 - mse: 0.7009 - val_loss: 1.8534 - val_mse: 0.9022\n",
      "Epoch 109/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6495 - mse: 0.6978 - val_loss: 1.8533 - val_mse: 0.9019\n",
      "Epoch 110/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6589 - mse: 0.7064 - val_loss: 1.8536 - val_mse: 0.9019\n",
      "Epoch 111/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6441 - mse: 0.6925 - val_loss: 1.8541 - val_mse: 0.9020\n",
      "Epoch 112/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6515 - mse: 0.6989 - val_loss: 1.8541 - val_mse: 0.9014\n",
      "Epoch 113/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6547 - mse: 0.7014 - val_loss: 1.8546 - val_mse: 0.9017\n",
      "Epoch 114/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6522 - mse: 0.6989 - val_loss: 1.8545 - val_mse: 0.9011\n",
      "Epoch 115/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6560 - mse: 0.7019 - val_loss: 1.8545 - val_mse: 0.9009\n",
      "Epoch 116/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6569 - mse: 0.7029 - val_loss: 1.8542 - val_mse: 0.9003\n",
      "Epoch 117/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6611 - mse: 0.7059 - val_loss: 1.8547 - val_mse: 0.9007\n",
      "Epoch 118/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6532 - mse: 0.6986 - val_loss: 1.8545 - val_mse: 0.9001\n",
      "Epoch 119/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6543 - mse: 0.6996 - val_loss: 1.8549 - val_mse: 0.9001\n",
      "Epoch 120/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6551 - mse: 0.6995 - val_loss: 1.8547 - val_mse: 0.8995\n",
      "Epoch 121/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6551 - mse: 0.6996 - val_loss: 1.8554 - val_mse: 0.8998\n",
      "Epoch 122/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6588 - mse: 0.7025 - val_loss: 1.8553 - val_mse: 0.8994\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6529 - mse: 0.6969 - val_loss: 1.8558 - val_mse: 0.8994\n",
      "Epoch 124/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6504 - mse: 0.6936 - val_loss: 1.8561 - val_mse: 0.8992\n",
      "Epoch 125/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6520 - mse: 0.6949 - val_loss: 1.8563 - val_mse: 0.8995\n",
      "Epoch 126/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6552 - mse: 0.6979 - val_loss: 1.8560 - val_mse: 0.8990\n",
      "Epoch 127/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6497 - mse: 0.6924 - val_loss: 1.8563 - val_mse: 0.8990\n",
      "Epoch 128/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6518 - mse: 0.6943 - val_loss: 1.8562 - val_mse: 0.8987\n",
      "Epoch 129/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6530 - mse: 0.6951 - val_loss: 1.8568 - val_mse: 0.8991\n",
      "Epoch 130/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6546 - mse: 0.6960 - val_loss: 1.8571 - val_mse: 0.8987\n",
      "Epoch 131/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6548 - mse: 0.6960 - val_loss: 1.8572 - val_mse: 0.8986\n",
      "Epoch 132/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6521 - mse: 0.6930 - val_loss: 1.8568 - val_mse: 0.8981\n",
      "Epoch 133/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6550 - mse: 0.6958 - val_loss: 1.8576 - val_mse: 0.8986\n",
      "Epoch 134/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6543 - mse: 0.6949 - val_loss: 1.8576 - val_mse: 0.8984\n",
      "Epoch 135/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6492 - mse: 0.6897 - val_loss: 1.8574 - val_mse: 0.8978\n",
      "Epoch 136/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6486 - mse: 0.6889 - val_loss: 1.8575 - val_mse: 0.8975\n",
      "Epoch 137/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6482 - mse: 0.6876 - val_loss: 1.8577 - val_mse: 0.8975\n",
      "Epoch 138/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6485 - mse: 0.6886 - val_loss: 1.8584 - val_mse: 0.8980\n",
      "Epoch 139/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6507 - mse: 0.6895 - val_loss: 1.8584 - val_mse: 0.8984\n",
      "Epoch 140/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6478 - mse: 0.6868 - val_loss: 1.8585 - val_mse: 0.8979\n",
      "Epoch 141/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6476 - mse: 0.6862 - val_loss: 1.8587 - val_mse: 0.8980\n",
      "Epoch 142/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6518 - mse: 0.6905 - val_loss: 1.8588 - val_mse: 0.8976\n",
      "Epoch 143/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6553 - mse: 0.6937 - val_loss: 1.8588 - val_mse: 0.8972\n",
      "Epoch 144/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6563 - mse: 0.6941 - val_loss: 1.8592 - val_mse: 0.8980\n",
      "Epoch 145/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6499 - mse: 0.6882 - val_loss: 1.8593 - val_mse: 0.8977\n",
      "Epoch 146/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6494 - mse: 0.6875 - val_loss: 1.8593 - val_mse: 0.8976\n",
      "Epoch 147/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6545 - mse: 0.6923 - val_loss: 1.8590 - val_mse: 0.8968\n",
      "Epoch 148/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6476 - mse: 0.6854 - val_loss: 1.8593 - val_mse: 0.8971\n",
      "Epoch 149/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6540 - mse: 0.6911 - val_loss: 1.8599 - val_mse: 0.8973\n",
      "Epoch 150/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6549 - mse: 0.6920 - val_loss: 1.8599 - val_mse: 0.8971\n",
      "Epoch 151/500\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 1.6571 - mse: 0.6938 - val_loss: 1.8597 - val_mse: 0.8969\n",
      "Epoch 152/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6528 - mse: 0.6897 - val_loss: 1.8600 - val_mse: 0.8973\n",
      "Epoch 153/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6470 - mse: 0.6837 - val_loss: 1.8599 - val_mse: 0.8973\n",
      "Epoch 154/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6521 - mse: 0.6886 - val_loss: 1.8602 - val_mse: 0.8975\n",
      "Epoch 155/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6513 - mse: 0.6879 - val_loss: 1.8604 - val_mse: 0.8972\n",
      "Epoch 156/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6487 - mse: 0.6847 - val_loss: 1.8603 - val_mse: 0.8970\n",
      "Epoch 157/500\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6494 - mse: 0.6855 - val_loss: 1.8607 - val_mse: 0.8970\n",
      "Epoch 00157: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cfc519160>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "X = [dataset[\"userId\"].to_numpy(), dataset[\"movieId\"].to_numpy()]\n",
    "y = dataset[\"rating\"].to_numpy()\n",
    "\n",
    "#TOFILL - remplir avec les best hyperparameters k et lambda (qu'on vient de découvrir)\n",
    "best_model=get_mf_bias_l2_reg_model(nb_users, nb_movies, k =30, lambda_ =0.0002)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1)\n",
    "\n",
    "best_model.fit(X, y, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# on entraine notre modèle sur TOUS les ratings connus - tout X - pour pouvoir faire des prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend the top-5 movies for the 10 first users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your retrained best model with optimal hyper parameters, compute the predictions for all the ratings that are not in the `dataset` for the 10 first users (indexes from 0 to 9). That means all the movies $i$ that these users $u \\in 0,\\ldots,9$ haven't rated, thus all the $u,i$ combinations that are not in the `dataset` dataframe rows.\n",
    "\n",
    "Order these predicted ratings for these users by decreasing order, and print out the 5 first ones, i.e. the ones that have the highest predicted ratings. Use the *movies.csv* file to print the real titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_for_user(model, user_id, dataset):\n",
    "    \"\"\"\n",
    "    Returns a list of the 5 movies that have the highest ratings among the unrated movies\n",
    "    of user `user_id`, along with a list of their predicted ratings.\n",
    "    \n",
    "    Input :\n",
    "        model : keras.models.Model : A trained matrix factorization model\n",
    "        user_id : int : The user id to use\n",
    "        dataset : DataFrame : The whole dataset, useful to find the movies \n",
    "            the user `user_id` has already rated\n",
    "    \n",
    "    Output :\n",
    "        five_best_movie_ids : list : The five movie ids among unrated movies by user `user_id` \n",
    "            that have the highest predicted ratings, in order\n",
    "        five_best_ratings : list : The corresponding five ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    #TOFILL\n",
    "    # faire une liste de tous les films que le user a pas vu\n",
    "    # récupérer les pairs user-film pas noté\n",
    "    \n",
    "    \n",
    "    return five_best_movie_ids, five_best_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at what is going on in the embedding space of the movies that we learnt. Our brain cannot picture anything beyond 3 dimensions, and we learnt high dimensional embeddings (k=15 or 30), so we are going to project the movies embeddings on a 2D plane, first with PCA, and then with another algorithm made for visualizing high dimensional spaces called t-sne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already studied PCA, it is a useful technique for dimensionality reduction, but also simply for visualization. Don't forget to scale your embeddings first. To access the embeddings values of your keras model, have a look at the *get_weights()* function.\n",
    "\n",
    "Compute a PCA on all your movies embeddings, get the 2 first principal components, and do a scatter plot of 20 randomly sampled movies on a 2D plane, where each movie is a point defined by the two values of the two first principal components of the PCA from its embedding. Add the titles to the plot, and try to see if you can interpret the axes of the PCA through to different movie genres, like in Figure 3 from the article *Matrix Factorization Techniques for Recommender Systems*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with t-sne, an algorithm specialized for visualizing high dimensional spaces, you can read more about it there : https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#TOFILL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-sne in general tends to preserve local similarities better than PCA. In any case, it's always interesting to try both for visualizing high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can export your embedding and upload them on https://projector.tensorflow.org/ to visualize the embeddings in 3D. You can also use the movies genres from the *movies.csv* file to make one plot for each movie genre and try to see if some parts of the embedding space are representative of a movie genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL PARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies to yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that ask you to rate 20 movies, then add your own ratings to the dataset, retrain the model, and compute your own top-5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rate_my_movies(my_user_id, dataset, nb_movies, nb_to_rate, movie_ids_map):\n",
    "    \"\"\"\n",
    "    Returns a dataframe in the same format as the dataset dataframe, with\n",
    "    ratings entered by the user for `nb_to_rate` random movies\n",
    "    \n",
    "    Input :\n",
    "        my_user_id : int : The user_id of the new ratings\n",
    "        dataset : DataFrame : The whole dataset \n",
    "        nb_movies : int : Number of unique movie ids\n",
    "        nb_to_rate : int : Number of movies to rate\n",
    "        movie_ids_map : dict : The mapping of original file userId to a new index starting at 0.\n",
    "    \n",
    "    Output : \n",
    "        my_ratings : DataFrame : A dataframe with the same column as `dataset` containing\n",
    "            the new ratings entered by the user\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    return my_ratings\n",
    "\n",
    "\n",
    "my_user_id = len(user_ids_map)\n",
    "\n",
    "my_ratings = rate_my_movies(my_user_id, dataset, nb_movies, 20, movie_ids_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_me = pd.concat([dataset, my_ratings], axis = 0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_with_me = [dataset_with_me[\"userId\"].to_numpy(), dataset_with_me[\"movieId\"].to_numpy()]\n",
    "y_with_me = dataset_with_me[\"rating\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model=get_mf_bias_l2_reg_model(nb_users + 1, nb_movies, 15, 1*10**-5)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1)\n",
    "\n",
    "best_model.fit(X_with_me, y_with_me, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_best_movie_ids, five_best_ratings =  get_top5_for_user(best_model, my_user_id, dataset)\n",
    "for i in range(5):\n",
    "    print('\\t' + ml_movie_id_to_title[ inverse_movie_ids_map[ five_best_movie_ids[i] ] ] + \n",
    "          '; predicted rating : ' + str(five_best_ratings[i]) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the movie embeddings to predict the movies genre with multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the goal of predicting missing rating, the matrix factorization techniques also produces vectorial representation of movies and users: their embeddings, what we just visualized for the movies. With a big enough dataset, these embeddings actually are good abstract representations of the movies and of the users, and can be reused as features for other tasks, such as classification.\n",
    "\n",
    "In the *movies.csv*, there is a column that gives the genres of each movie. Let's try to predict the genres of the movies from the embeddings we learnt. As you can see, each movie can have more than one genre, so in classification terms, more than one class. We can achieve that with *multilabel classification*. You can read more about it there: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "Load the movies genre, encode them as binary classes and use the classes imported below to train a multilabel classifier that uses the movie embeddings as features, and the movie genres as classes. Use the *OneVsRestClassifier* with a simple *LinearSVC* without any hyper-parameter tuning. Finally print the test accuracy, F1, precision and recall for each class, as well as the number of time each class appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rare classes, you should get a very high accuracy, with a very low F1. Indeed these classes are really imbalanced : there are a few positives, hence the classifier is largely biased toward the negatives, and rarely predict a positive for these classes. This is why accuracy is generally a bad measure with imbalanced dataset : the high number of true negatives makes the accuracy number high, while our model is actually barely capable of predicting true positives.\n",
    "\n",
    "Let's compare our classifier performance with a *DummyClassifier*, the dummy classifier takes the ratio $r = \\frac{nb\\_positives}{nb\\_positives + nb\\_negatives}$ as the probability to predict a positive, and then do it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, simply respecting the class balance, even at random, produces better F1 on most classes. One way to compensate for class imbalance is to tell the classifier to weight more the true samples at training time, accordingly with the ratio $r$ between true and false samples. With scikit-learn SVM implementation, you can use the argument *class_weight* for setting the weight of the positive and negative samples at training time. See : https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "But if you just want to set the class weights accordingly with the ratio between positives and negatives, you can just set *class_weight = ‘balanced’*. Test it with the LinearSVC classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is now much better than with the dummy classifier, however is is still not very convincing. This is quite normal given the size of the dataset we are using, which is pretty small to get really meaningful embeddings. But with bigger datasets, reusing embeddings as features for auxiliary tasks such as classification is actually a very effective way of doing so when there is no other informations about the items we try to classify. Here the items are the movies, the dataset doesn't provide more information about them, but one could imagine fetching from internet textual descriptions of the movies and use them as features alongside the embeddings to improve the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the different SGD algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the notebook we used the 'adam' `optimizer` to train our model, which is a variation of SGD. Keras proposes different variations of SGD: https://keras.io/optimizers/ . This article gif images gives an intuitive view of their different behavior : https://medium.com/@ramrajchandradevan/the-evolution-of-gradient-descend-optimization-algorithm-4106a6702d39\n",
    "\n",
    "Try a few ones with our model and see how the training and testing loss evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the global bias $\\mu$  parameter to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we didn't added the global bias $\\mu$ to our model yet (Equations (4-5) from Koren's paper). Use your best google skills to find a way to add an embedding layer that does that.\n",
    "\n",
    "Hint : Use a constant `Input` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own Stochastic Gradient Descent for Matrix Factorization with numpy instead of Keras (very optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know everything to implement your own matrix factorization SGD model, all with numpy arrays. Start without the biases again, and without mini-batches. The gradient update equations are described in page 4 of Koren's paper. Let's initialize your $p$ and $q$ embeddings with a gaussian sampling. Print the RMSE at the beginning of each epoch, and finally compute the RMSE of your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "\n",
    "P = normal(size = (nb_users,k))\n",
    "Q = normal(size = (nb_movies,k))\n",
    "\n",
    "gamma = 0.1\n",
    "lambda_ = 0.00001\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for j in range(train.shape[0]):\n",
    "        u = train['userId'].iloc[j]\n",
    "        i = train['movieId'].iloc[j]\n",
    "        r_ui = train['rating'].iloc[j]\n",
    "        \n",
    "        #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
